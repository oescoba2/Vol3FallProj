{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "import joblib\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry_convert\n",
    "import pycountry\n",
    "import time\n",
    "import wbgapi as wb\n",
    "import warnings\n",
    "\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_available = True\n",
    "except ImportError:\n",
    "    warnings.warn(message=\"Unable to import optuna. Bayesian optimization is not available.\")\n",
    "    optuna_available = False\n",
    "\n",
    "try:\n",
    "    import optunahub\n",
    "    optunahub_available = True\n",
    "except ImportError:\n",
    "    warnings.warn(message=\"Unable to import optunahub. Auto-sampler not available.\")\n",
    "    optunahub_available = False\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300 \n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "IMG_DIR = 'images/'\n",
    "MODELS_DIR = 'saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define WorldBank Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WbDataPipeline():\n",
    "    '''\n",
    "    A class to pull and clean World Bank data for use in the happiness dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indicators, year, impute=True, missing_countries=False) -> None:\n",
    "        '''\n",
    "        Creates the World Bank data pipeline\n",
    "\n",
    "        Parameters:\n",
    "        indicators (list): list of World Bank indicator codes\n",
    "        year (int): year to pull data from. We used 2022\n",
    "        impute (bool): whether to impute missing data\n",
    "        missing_countries (bool): whether to include countries not in the happiness dataset\n",
    "        '''\n",
    "\n",
    "        self.indicators = indicators\n",
    "        self.year = year\n",
    "\n",
    "        # Data from the happiness dataset had special formatting that we had to extract by hand\n",
    "        self.happiness_data = pd.read_csv(DATA_DIR + 'happiness/happiness.csv').drop('Country', axis=1)\n",
    "        self.valid_countries = self.happiness_data['ISO_A3'].unique()\n",
    "        self.missing_countries = missing_countries\n",
    "        self.impute = impute\n",
    "\n",
    "        # Set the world bank database to the World Development Indicators\n",
    "        wb.db = 2\n",
    "        self.data = self.pull_data()\n",
    "    \n",
    "    def pull_data(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        Pulls the World Bank data and merges it with the happiness data\n",
    "        '''\n",
    "        \n",
    "        # Pull the data\n",
    "        print(f\"Pulling {len(self.indicators)} indicators from World Bank data...\")\n",
    "        features = wb.data.DataFrame(self.indicators, time=self.year)\n",
    "        print(\"Done\")\n",
    "        features = features.reset_index()\n",
    "        features = features.infer_objects()\n",
    "        features = features.rename(columns={features.columns[0]: 'ISO_A3'})\n",
    "\n",
    "        # If we only want countries in the happiness dataset\n",
    "        if not self.missing_countries:\n",
    "            features = features[features['ISO_A3'].isin(self.valid_countries)]\n",
    "\n",
    "        if self.impute:\n",
    "            features = self.impute_numeric_data(features)\n",
    "\n",
    "        # Merge with happiness data\n",
    "        if not self.missing_countries:\n",
    "            merged = pd.merge(features, self.happiness_data, on='ISO_A3')\n",
    "            return merged\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def impute_numeric_data(self, data) -> pd.DataFrame :\n",
    "        '''This code fills in missing numerical data with the mean of its 5 nearest neighbors\n",
    "        as determined by its nonmissing numerical data. No categorical features are \n",
    "        touched.\n",
    "        \n",
    "        Parameters:\n",
    "        data (pd.DataFrame): the data to impute\n",
    "        '''\n",
    "        imputed_data = KNNImputer().fit_transform(data.iloc[:, 1:])\n",
    "        data.iloc[:, 1:] = imputed_data\n",
    "        return data\n",
    "    \n",
    "    def get_data(self) -> pd.DataFrame: \n",
    "        '''\n",
    "        Getter for the data\n",
    "        '''\n",
    "\n",
    "        if self.data is None:\n",
    "            self.data = self.pull_data()\n",
    "        return self.data\n",
    "\n",
    "    def check_missing(self, threshold) -> pd.Series:\n",
    "        '''\n",
    "        Generates a report of how many features are less than a certain percentage complete\n",
    "\n",
    "        Parameters:\n",
    "        threshold (float): the threshold for what percentage of the data must be complete\n",
    "        '''\n",
    "\n",
    "        # Sum up the number of missing values\n",
    "        nans = self.data.isna().sum()\n",
    "\n",
    "        # We are more interested in the complete percentage, but this code was originally written for missing percentage\n",
    "        threshold = 1 - threshold\n",
    "        nthreshold = np.round(self.data.shape[0] * threshold)\n",
    "        cols = nans[nans > nthreshold]\n",
    "        \n",
    "        # Print the report\n",
    "        print(f\"The following features are less than {100*(1-threshold)}% complete:\")\n",
    "        for col in cols.index:\n",
    "            pcomplete = 1 - (nans[col] / self.data.shape[0])\n",
    "            print(f\"   {col}: {pcomplete*100}% complete\") \n",
    "        return cols\n",
    "    \n",
    "    def check_percent_complete(self) -> pd.Series:\n",
    "        '''\n",
    "        Returns the perentage of features that are complete\n",
    "        '''\n",
    "        \n",
    "        nans = self.data.isna().sum()\n",
    "        print(f\"Percent complete: {self.get_percent_complete()}\")\n",
    "        return 1 - (nans / self.data.shape[0])\n",
    "    \n",
    "def clean_features(write_csv=None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to get all of the feature data and generate csv containing\n",
    "    the feature id, its description, and how complete that feature is.\n",
    "    This is usefull for selecting features that dono have too many missing values\n",
    "\n",
    "    Parameters:\n",
    "    write_csv (str): the file to write the data to\n",
    "    '''\n",
    "\n",
    "    entries = []\n",
    "    indicators = wb.series.list()\n",
    "    \n",
    "    # Get all ids and values for all of the indicators\n",
    "    ids = [(str(indicator['id']), str(indicator['value'])) for indicator in indicators]\n",
    "\n",
    "    # This can take a while so a progress bar is nice\n",
    "    pbar = tqdm(total=len(ids), position=0, leave=True)\n",
    "    \n",
    "    for id, value in ids:\n",
    "        dp = WbDataPipeline([id], 2022, impute=False)\n",
    "        complete_percent = dp.get_percent_complete()[id]\n",
    "        entries.append([id, value, complete_percent])\n",
    "        pbar.update()\n",
    "    pbar.close()\n",
    "\n",
    "    # Return the data as a dataframe\n",
    "    entries = pd.DataFrame(entries, columns=['id', 'value', 'complete_percent'])\n",
    "\n",
    "    # Write the feature data to a file\n",
    "    if write_csv is not None:\n",
    "        entries.to_csv(write_csv)\n",
    "\n",
    "    return entries\n",
    "\n",
    "def generate_wb_dataset(complete_percent=1.0, write_csv=None, missing_countries=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Function to generate a dataset from the World Bank data\n",
    "    '''\n",
    "\n",
    "    # Read from the feature data file\n",
    "    feature_data = pd.read_csv('feature_data.csv')\n",
    "    feature_data = feature_data.infer_objects()\n",
    "    features = feature_data[feature_data['complete_percent'] >= complete_percent]['id']\n",
    "    \n",
    "    # Get only countries satisfying the missing countries condition\n",
    "    dp = WbDataPipeline(features, 2022, missing_countries=missing_countries)\n",
    "\n",
    "    if write_csv is not None:\n",
    "        dp.get_data().to_csv(write_csv)\n",
    "\n",
    "    return dp.get_data()\n",
    "\n",
    "# We encountered different naming conventions for countries in some of our experimental datasets.\n",
    "# This function standardizes some of thse names so we can use their ISO A3 codes\n",
    "\n",
    "def name_change(df) -> pd.DataFrame:\n",
    "        '''\n",
    "        Replace unusual country names and get ISO A3 codes\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): the dataframe\n",
    "        '''\n",
    "        \n",
    "        # Define a dictionary of all the name changes we encountered\n",
    "        name_changes = {'Bolivia (Plurinational State of)' : 'Bolivia',\n",
    "        'Democratic Republic of the Congo' : 'Congo, The Democratic Republic of the',\n",
    "        'Iran (Islamic Republic of)':'Iran',\n",
    "        'Micronesia (Federated States of)' : 'Micronesia, Federated States of',\n",
    "        'Republic of Korea' : 'Korea, Republic of',\n",
    "        'Swaziland' : 'Eswatini',\n",
    "        'The former Yugoslav republic of Macedonia' : 'North Macedonia',\n",
    "        'Turkey' : 'TÃ¼rkiye',\n",
    "        'Venezuela (Bolivarian Republic of)' : 'Venezuela, Bolivarian Republic of',\n",
    "        'Taiwan Province of China' : 'Taiwan',\n",
    "        'Kosovo' : 'Serbia',\n",
    "        'North Cyprus' : 'Cyprus',\n",
    "        'Russia' : 'Russian Federation',\n",
    "        'Hong Kong S.A.R. of China' : 'Hong Kong',\n",
    "        'Ivory Coast' : 'CI',\n",
    "        'Palestinian Territories' : 'PS',\n",
    "        'Eswatini, Kingdom of' : 'SZ',}\n",
    "        \n",
    "        # Use pycountry to get the ISO A3 code for each country\n",
    "        def get_country_code(country_name):\n",
    "            if country_name[-1] == '*':\n",
    "                country_name = country_name[:-1]\n",
    "            if country_name in name_changes:\n",
    "                country_name = name_changes[country_name]\n",
    "            try:\n",
    "                return pycountry.countries.get(country_name).alpha_3\n",
    "            except:\n",
    "                try:\n",
    "                    return pycountry.countries.lookup(country_name).alpha_3\n",
    "                except:    \n",
    "                    # If this is the case we need to manually enter the unusual country name\n",
    "                    raise ValueError(f\"No ISO code associated with country {country_name}\")\n",
    "        \n",
    "        df['ISO_A3'] = df['Country'].apply(get_country_code)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Plotter for Happiness v. GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gdp_happiness(ax, logscale=False) -> None:\n",
    "    '''\n",
    "    Generate the plot of GDP vs Happiness score\n",
    "\n",
    "    Parameters:\n",
    "    ax (matplotlib.pyplot.axis): the axis to plot on\n",
    "    logscale (bool): whether to plot the data on a log scale\n",
    "    '''\n",
    "    \n",
    "    # Pull the data\n",
    "    wb.db = 2\n",
    "    dp = WbDataPipeline([\"NY.GDP.PCAP.PP.CD\"], 2022, impute=False)\n",
    "    data = dp.get_data()\n",
    "    data = data.rename(columns={\"NY.GDP.PCAP.PP.CD\": \"GDP\"})\n",
    "\n",
    "    # Manually enter the GDP for countries that are missing. Sourced from the World Bank but not in the dataset\n",
    "    data.loc[data[\"ISO_A3\"] == \"TKM\", \"GDP\"] = 8792.55\n",
    "    data.loc[data[\"ISO_A3\"] == \"VEN\", \"GDP\"] = 3421\n",
    "    data.loc[data[\"ISO_A3\"] == \"YEM\", \"GDP\"] = 698.95\n",
    "\n",
    "    # Whether to plot and fit models on log scale\n",
    "    if logscale:\n",
    "        data[\"GDP\"] = np.log(data[\"GDP\"])\n",
    "\n",
    "    # Setup data for regression\n",
    "    xs = np.linspace(data['GDP'].min(), data['GDP'].max(), 1000)\n",
    "    X = data['GDP'].values.reshape(-1, 1)\n",
    "    Y = data['Happiness score'].values.reshape(-1, 1)\n",
    "\n",
    "    # Fit the linear model\n",
    "    model1 = LinearRegression().fit(X, Y)\n",
    "    lin_preds = model1.predict(X)\n",
    "    print(\"Linear model MSE:\", mean_squared_error(Y, lin_preds))\n",
    "    print(\"Linear model R^2:\", r2_score(Y, lin_preds))\n",
    "\n",
    "    # Define a log model\n",
    "    def log_model(th, x=X):\n",
    "        return th[0] * np.log(x) + th[1]\n",
    "    \n",
    "    # Fit the log model\n",
    "    res = minimize(lambda th : np.mean((log_model(th) - Y)**2), [1, 1])\n",
    "    log_params = res.x\n",
    "    log_preds = log_model(log_params)\n",
    "    print(\"Log model MSE:\", mean_squared_error(Y, log_preds))\n",
    "    print(\"Log model R^2:\", r2_score(Y, log_preds))\n",
    "\n",
    "    # Plot all of the countries colored by continent\n",
    "    colors = {'EU': 'blue', 'AS': 'orange', 'AF': 'red', 'NA': 'green', 'SA': 'yellow', 'OC': 'purple'}\n",
    "    for country in data['ISO_A3']:\n",
    "        color = colors[pycountry_convert.country_alpha2_to_continent_code(pycountry.countries.get(alpha_3=country).alpha_2)]\n",
    "        ax.scatter(data.loc[data['ISO_A3'] == country, 'GDP'], data.loc[data['ISO_A3'] == country, 'Happiness score'], marker='o', color=color, s=4)\n",
    "\n",
    "    # Plot the models\n",
    "    ax.plot(xs, model1.predict(xs.reshape(-1,1)), color='black', label='Linear Model')\n",
    "    ax.plot(xs, log_model(log_params, xs.reshape(-1,1)), color='grey', label='Log Model', linestyle='--')    \n",
    "\n",
    "    # Set the labels\n",
    "    ax.set_xlabel('GDP per Capita (USD)', fontsize=6)\n",
    "    ax.set_ylabel('Happiness Score', fontsize=6)\n",
    "    ax.set_title('Happiness Score vs GDP per Capita', fontsize=10)\n",
    "    ax.set_xticks\n",
    "\n",
    "    # Create a custom legend\n",
    "    custom_points = [Line2D([0], [0], color='black', label='Linear Model', linestyle='-'),\n",
    "                    Line2D([0], [0], color='grey', label='Log Model', linestyle='--'),\n",
    "                    Line2D([0], [0], color='red', marker='o', label='Africa', linestyle=''),\n",
    "                    Line2D([0], [0], color='orange', marker='o', label='Asia', linestyle=''),\n",
    "                    Line2D([0], [0], color='yellow', marker='o', label='Souh America', linestyle=''),\n",
    "                        Line2D([0], [0], color='green', marker='o', label='North America', linestyle=''),\n",
    "                        Line2D([0], [0], color='blue', marker='o', label='Europe', linestyle=''),\n",
    "                        Line2D([0], [0], color='purple', marker='o', label='Oceania', linestyle=''),]\n",
    "    ax.legend(handles=custom_points, loc=\"lower right\", fontsize=6)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ```GeoPlotter``` for worldmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoPlotter():\n",
    "    '''\n",
    "    Class for plotting data on a world map\n",
    "    '''\n",
    "\n",
    "    def __init__(self, df=None) -> None:\n",
    "        '''\n",
    "        Initializes the GeoPlotter\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): the dataframe to plot\n",
    "        '''\n",
    "\n",
    "        if 'ISO_A3' not in df.columns:\n",
    "            raise ValueError(\"This dataframe does not have an ISO_A3 column. Please use the DataPipeline change_name transform to prepare it for plotting\")\n",
    "        else: \n",
    "            self.df = df\n",
    "\n",
    "        world = gpd.read_file(DATA_DIR + 'worldmap.gpkg')\n",
    "    \n",
    "        # Bug in geopandas see https://github.com/geopandas/geopandas/issues/1041\n",
    "        # These values are set wrong in the world map data file\n",
    "        world.loc[world['NAME_EN'] == 'France', 'ISO_A3'] = 'FRA'\n",
    "        world.loc[world['NAME_EN'] == 'Norway', 'ISO_A3'] = 'NOR'\n",
    "        world.loc[world['NAME_EN'] == 'Somaliland', 'ISO_A3'] = 'SOM'\n",
    "        world.loc[world['NAME_EN'] == 'Kosovo', 'ISO_A3'] = 'RKS'\n",
    "\n",
    "        # Merge the data with the world map\n",
    "        self.merged = pd.merge(world, self.df, on='ISO_A3')\n",
    "        self.no_data = world[~world['ISO_A3'].isin(self.df['ISO_A3'])]\n",
    "\n",
    "    def plot(self, col_name, ax=None, year=None, title=None, save_img:bool=False, vmin:float=None, vmax:float=None, img_name:str='fig.pdf') -> None:\n",
    "        '''\n",
    "        Plots the data from the specified column\n",
    "\n",
    "        Parameters:\n",
    "        col_name (str): the column to plot\n",
    "        ax (matplotlib.pyplot.axis): the axis to plot on\n",
    "        year (str): the year to plot from (we used 2022 in our experimets)\n",
    "        title (str): the title of the plot\n",
    "        '''\n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(1, figsize=(10, 4))\n",
    "        # Make sure data is set\n",
    "        if year is None:\n",
    "            df = self.merged\n",
    "        else:\n",
    "            df = self.merged[self.merged['Year'].astype(str) == year]\n",
    "\n",
    "        if title is None:\n",
    "            title = col_name\n",
    "\n",
    "        # Plot the data and color missing countries grey\n",
    "        df.plot(column=col_name, edgecolor=\"black\", linewidth=0.2, ax=ax, legend=True, vmin=vmin, vmax=vmax, legend_kwds={'label': 'Happiness Score', 'aspect': 30, 'shrink': 0.8})\n",
    "        self.no_data.plot(ax=ax, edgecolor=\"grey\", linewidth=0.2, color='lightgrey', legend=True)\n",
    "        \n",
    "        # Format the plot\n",
    "        ax.set_title(title, fontsize=16)\n",
    "        ax.set_axis_off()\n",
    "        fig = ax.get_figure()\n",
    "        cax = fig.axes[1]\n",
    "        cax.set_ylabel(col_name)\n",
    "\n",
    "        if save_img:\n",
    "            plt.savefig(img_name, format='pdf')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def animate(self, col_name, video_name)-> None:\n",
    "        '''\n",
    "        Animates updates of time series data. We never used this for the final product, but we used it in experiments\n",
    "\n",
    "        Parameters:\n",
    "        col_name (str): the column to animate\n",
    "        video_name (str): the name of the video to save\n",
    "        '''\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 7))\n",
    "\n",
    "        # Updata the animation for each year\n",
    "        def update(year):\n",
    "            df = self.merged[self.merged['Year'].astype(str) == str(year)]\n",
    "            df.plot(column=col_name, ax=ax)\n",
    "            ax.set_title(col_name + \"\\n\" + str(year))\n",
    "\n",
    "        # Write the video to file\n",
    "        animation.writer = animation.writers['ffmpeg']\n",
    "        ani = animation.FuncAnimation(fig, update, \n",
    "            frames=sorted(set(self.df['Year'])),\n",
    "            interval=self.df['Year'].max() - self.df['Year'].min() / 10)\n",
    "        ani.save(video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ```FeatureSeparator``` to get factors on happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSeparator():\n",
    "    '''\n",
    "    Class for determining what features influence happiness independently of GDP\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, similarity_metric=\"cos\", alpha=1) -> None:\n",
    "        '''\n",
    "        Initializes the FeatureSeparator\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): the data to analyze\n",
    "        similarity_metric (str): the similarity metric to use. Either \"cos\" for cosine similarity or \"mi\" for mutual information\n",
    "        alpha (float): how much to weight the diffeence of GDP and happiness\n",
    "        '''\n",
    "        self.data = data\n",
    "\n",
    "        # Need cosine similarity or mutual information\n",
    "        if similarity_metric not in [\"cos\", \"mi\"]:\n",
    "            raise ValueError(\"Invalid similarity metric. Use 'cos' for cosine similarity or 'mi' for mutual information.\")\n",
    "\n",
    "\n",
    "        self.similarity_metric = similarity_metric\n",
    "\n",
    "        if similarity_metric == \"cos\":\n",
    "            self.similarity = np.matmul\n",
    "        else:\n",
    "            self.similarity = lambda X, Y : mutual_info_regression(X, Y, random_state=3)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_separator(self, X, Y) -> np.array:\n",
    "        '''\n",
    "        Find a distribution D* that maximizes similarity with X and minimizes similarity with Y\n",
    "\n",
    "        Parameters:\n",
    "        X (np.array): the first distribution\n",
    "        Y (np.array): the second distribution\n",
    "        '''\n",
    "\n",
    "        # Normalize the data\n",
    "        X = X.reshape(-1,1) / np.linalg.norm(X)\n",
    "        Y = Y.reshape(-1,1) / np.linalg.norm(Y)\n",
    "\n",
    "        if self.similarity_metric == \"cos\":\n",
    "            X = np.ravel(X) \n",
    "            Y = np.ravel(Y)\n",
    "\n",
    "        # Define the objective function\n",
    "        def obj(D):\n",
    "            return -np.abs(self.similarity(X, D)) + self.alpha * np.abs(self.similarity(Y, D))\n",
    "\n",
    "        # Initial guess\n",
    "        D = X.reshape(-1)\n",
    "        D = D / np.linalg.norm(D)\n",
    "        \n",
    "        # Constrain the distribution to have norm 1 (mutual information depends on parameter size)\n",
    "        constraints = [{'type': 'eq', 'fun': lambda x: np.linalg.norm(x) - 1}]\n",
    "\n",
    "        res = minimize(obj, D, constraints=constraints, tol=1e-14)\n",
    "\n",
    "        # Print the similarity of the distributions        \n",
    "        print(\"Similarity of D with X\", self.similarity(X, np.ravel(res.x)))\n",
    "        print(\"Similarity of X with X\", self.similarity(X, np.ravel(X)))\n",
    "        return res.x\n",
    "\n",
    "    def get_most_similar_feature(self, D, F, X, Y, n=None):\n",
    "        '''\n",
    "        Find which features are most similar to the separator D* in the dataset\n",
    "\n",
    "        Parameters:\n",
    "        D (np.array): the separator\n",
    "        F (pd.DataFrame): the data to analyze\n",
    "        X (np.array): the first distribution\n",
    "        Y (np.array): the second distribution\n",
    "        n (int): the number of features to return\n",
    "        '''\n",
    "\n",
    "        # Normalize the data\n",
    "        if n is None:\n",
    "            n = F.shape[1]\n",
    "        F_arr = np.array(F)\n",
    "        F_arr = F_arr / np.linalg.norm(F_arr, axis=0)\n",
    "\n",
    "        if self.similarity_metric == \"cos\":\n",
    "            F_arr = F_arr.T\n",
    "\n",
    "        # Get the similarity of each feature to D*\n",
    "        simD = self.similarity(F_arr, D)\n",
    "        abs_simD = np.abs(simD)\n",
    "        keys = np.argsort(abs_simD)[::-1][:n]\n",
    "        \n",
    "        simDs = simD[keys]\n",
    "        simXs = self.similarity(F_arr, X)[keys]\n",
    "        simYs = self.similarity(F_arr, Y)[keys]\n",
    "        return F.columns[keys], simDs, simXs, simYs\n",
    "\n",
    "    def get_separating_feature(self, col1, col2, n=None):\n",
    "        '''\n",
    "        Run the whole procecss of finding D* and then the most similar features to D*\n",
    "\n",
    "        Parameters:\n",
    "        col1 (str): the first column to compare\n",
    "        col2 (str): the second column to compare\n",
    "        n (int): the number of features to return\n",
    "        '''\n",
    "        data = self.data\n",
    "\n",
    "        # Get the separator\n",
    "        D = self.get_separator(data[col1].values, data[col2].values)\n",
    "        F = data.drop([col1, col2], axis=1)\n",
    "        X = data[col1] \n",
    "        Y = data[col2]\n",
    "\n",
    "        # Make sure the data is normalized\n",
    "        X /= np.linalg.norm(X)\n",
    "        Y /= np.linalg.norm(Y)\n",
    "\n",
    "        # Get the most similar features\n",
    "        return self.get_most_similar_feature(D, F, X, Y, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ```engineering``` to perform feature engineering & extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class engineering():\n",
    "    \"\"\"This class is meant to perform feature engineering by either \n",
    "    selecting the best features according to mutual information or \n",
    "    generate new features using PCA (or a combination of both).\n",
    "    \n",
    "    Attributes:\n",
    "        - X_tr: an array holding the training array of features\n",
    "        - y_tr: an array holding the training targets\n",
    "        - select_kfeats: a number specifying the number of features to\n",
    "                         select\n",
    "        - mut_info_kneighbors: the number of neighbors to use to compute\n",
    "                               mutual information\n",
    "        - pca_comp: the number of principal components to compute\n",
    "        - pca_desired_var: the desired variance the principal components\n",
    "                           should have\n",
    "        - X_tr_scaled_selected: an array holding the selected and scaled\n",
    "                                training array\n",
    "        - features_selected: a list containing the names of the features \n",
    "                             selected\n",
    "        - X_tr_pca: an array holding the transformed X_tr array into the \n",
    "                    principal components containing only a certain number\n",
    "                    of components needed to reach a desired variance.\n",
    "\n",
    "    Methods:\n",
    "        - __init__(): the constructor for the class\n",
    "        - get_bestk_features(): method to select a certain number of features\n",
    "        - get_pca_features(): method to select a certain number of \n",
    "                              principal components\n",
    "        - get_X_test_pca(): method to transform a test set of features into \n",
    "                            the space of principal components\n",
    "                        \n",
    "    \n",
    "    Hidden Attributes:\n",
    "        - _scaler: an instance of StandardScaler used to scale data. This \n",
    "                   is fitted with data once select_kfeats or get_pca_feat-\n",
    "                   res are called.\n",
    "        - _pca: an instance of PCA used to compute principal components. It\n",
    "                is fitted once get_pca_features is called.\n",
    "        - _X_tr_scaled: an array holding the scaled X_tr array using _scaler\n",
    "        - _selector: an instance of SelectKBest that is fitted at the time\n",
    "                     select_features is called\n",
    "        _ _pca_mask: a Boolean array holding the True values of the principal\n",
    "                     components that are needed to achieve a desired variance\n",
    "\n",
    "    Hidden Methods:\n",
    "        - _mutual_scorer(): method to compute the mutual information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_tr:NDArray, y_tr:ArrayLike, select_kfeats:int=20, \n",
    "                 mut_info_kneighbors:int=6, pca_comp:int=45,\n",
    "                 pca_desired_var:float=0.95) -> None:\n",
    "        \n",
    "        \"\"\"The constructor for the class. This functions accepts arguments\n",
    "        and creates the attributes for the class.\n",
    "        \n",
    "        Parameters:\n",
    "            - X_tr (NDArray): an array containing the features that will\n",
    "                              be used for training.\n",
    "            - y_tr (ArrayLike): an array containg the targets that will be\n",
    "                                used for training.\n",
    "            - select_kfeats (int): the number of features to select using\n",
    "                                   mutual_info_regression. Defaulted to 20\n",
    "            - mut_info_kneighbors (int): the number of neighbors (K nearest)\n",
    "                                         to use for approximating mutual \n",
    "                                         information. Defaulted to 6.\n",
    "            - pca_comp (int): the number of principal components to compute\n",
    "                              (using SVD). Defaulted to 45.\n",
    "            - pca_desired_var (float): the desired variance that the chosen\n",
    "                                       number principal components must have.\n",
    "                                       Defauled to 0.95. Value must be bet-\n",
    "                                       ween 0.1 and 1.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.X_tr = X_tr\n",
    "        self.y_tr = y_tr\n",
    "        self.select_kfeatures = select_kfeats\n",
    "        self.mut_info_kneighbors = mut_info_kneighbors\n",
    "        self.pca_components = pca_comp\n",
    "        self.pca_desired_var = pca_desired_var\n",
    "        self._scaler = StandardScaler()\n",
    "        self._pca = PCA(n_components=pca_comp)\n",
    "\n",
    "        # Check input\n",
    "        if (pca_desired_var > 1) or (pca_desired_var < 0.1):\n",
    "            raise ValueError(\"Desired PCA variance cannot be greater than 1 or less than 0.1!\")\n",
    "    \n",
    "    def _mutual_scorer(self, X:NDArray, y:ArrayLike) -> ArrayLike:\n",
    "        \"\"\"This function is meant to be called when selecting features\n",
    "        according to mutual_info_regression. It allows the user to \n",
    "        give the argument of number of neighbors to use.\n",
    "\n",
    "        Parameters:\n",
    "            - X (NDArray): an array X that will have its mutual informa-\n",
    "                           tion to y computed\n",
    "            - y (ArrayLike): an array y used as the target with which\n",
    "                             to measure the mutual information of X\n",
    "        \n",
    "        Returns:\n",
    "            - (ArrayLike): the computed mutual information estimation\n",
    "        \"\"\"\n",
    "\n",
    "        return mutual_info_regression(X, y, n_neighbors=self.mut_info_kneighbors)\n",
    "\n",
    "    def get_bestk_features(self) -> None:\n",
    "        \"\"\"This function computes the mutual information between the stored\n",
    "        training feature array X_tr and training target array y. It then\n",
    "        selects a predetermined number of components and stores them as an\n",
    "        attribute including the names.\n",
    "\n",
    "        Parameters:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        self._X_tr_scaled = self._scaler.fit_transform(X=self.X_tr)\n",
    "        self._selector = SelectKBest(score_func=self._mutual_scorer, k=self.select_kfeatures).fit(X=self._X_tr_scaled, y=self.y_tr)\n",
    "        self.features_selected = self.X_tr.columns[self._selector.get_support()]\n",
    "        self.X_tr_mut_info = self._X_tr_scaled[:, self._selector.get_support()]\n",
    "\n",
    "    def get_X_test_mut_info(self, X_ts:NDArray) -> NDArray:\n",
    "        \"\"\"This function accepts an NDArray and returns the transformation of\n",
    "        the array into the already created mutual info selector.\n",
    "        \n",
    "        Parameters:\n",
    "            - X_ts (NDArray): an array containing the features that will be \n",
    "                              used for testing.\n",
    "        \n",
    "        Returns:\n",
    "            - (NDArray): the transformed test features arrays with features \n",
    "                         already selected.\n",
    "        \"\"\"\n",
    "\n",
    "        self._scaler.fit(X=self.X_tr)\n",
    "        return self._selector.transform(X_ts)\n",
    "\n",
    "    def get_pca_features(self) -> None:\n",
    "        \"\"\"This function performs PCA and saves the number of principal\n",
    "        components needed to reach a predetermined desired variance. If\n",
    "        it cannot reach the desired variance, the function raises an\n",
    "        Exception.\n",
    "\n",
    "        Parameters:\n",
    "            - None\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        self._X_tr_scaled = self._scaler.fit_transform(X=self.X_tr)\n",
    "        X_pca = self._pca.fit_transform(X=self._X_tr_scaled)\n",
    "        cum_sum = np.cumsum(self._pca.explained_variance_ratio_)          # Get the cumulative sum of the variance of each component\n",
    "\n",
    "        # Check that the desired variance is actually met by the chosen number of components\n",
    "        if cum_sum.max() < self.pca_desired_var:\n",
    "            raise Exception(f\"Desired variance is {self.pca_desired_var} but using {self.pca_components} components results in {cum_sum.max():.5f}\")\n",
    "        \n",
    "        if cum_sum.min() > self.pca_desired_var:\n",
    "            raise Exception(f\"Minimum variance of PCA is {cum_sum.max():.5f} which is greater than {self.pca_desired_var}. Please specify a greater value.\")\n",
    "        \n",
    "        self._pca_mask = (cum_sum >= self.pca_desired_var)                # Save the mask\n",
    "        self.X_tr_pca = X_pca[:, ~self._pca_mask]          # Save the principal components needed to achieve desired variance as an attribute\n",
    "\n",
    "    def get_X_test_pca(self, X_ts:NDArray) -> NDArray:\n",
    "        \"\"\"This function accepts an NDArray and returns the transformation of\n",
    "        the array into the already created principle component space.\n",
    "\n",
    "        Parameters:\n",
    "            - X_ts (NDArray): an array containing the features that will be \n",
    "                              used for testing.\n",
    "        \n",
    "        Returns:\n",
    "            - (NDArray): the transformed features array in the space of the \n",
    "                         computed principal components\n",
    "        \"\"\"\n",
    "\n",
    "        return (self._pca.transform(X=X_ts))[:, ~self._pca_mask]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ```Model``` Class for making ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"This class is meant to create a user defined model. It allows the user to specify\n",
    "    the model type, estimator choice, hyperparameters, and hyperparameter tuning strategy as\n",
    "    well as other important options. The user can make various types of models that can\n",
    "    be used trained and hypertuned or just instantiated. See the docstring of each method\n",
    "    for more information.\n",
    "\n",
    "    The class contains hidden attributes that are used to specify hyperparameters or\n",
    "    hyperparameter ranges or step sizes. These are divided by model choice and have\n",
    "    distinction by model choice. 'reg' is for regression and 'clf' is for classification.\n",
    "    There are also hidden methods that are used for hyperparameter tuning using optuna.\n",
    "    Wherever needed, estimator type is specified too.\n",
    "\n",
    "    Refer to all of the docstrings for documentations on the class, attributes methods,\n",
    "    and hidden attributes for more information.\n",
    "\n",
    "    Attributes\n",
    "        - model_choice: the choice of model to make. The two options are\n",
    "                            - 'clf' for classifier\n",
    "                            - 'reg' for regression.\n",
    "                        This is defaulted to 'reg'.\n",
    "        - est_type: the type of estimator to use for a given model choice.\n",
    "                      This is defaulted to 'rdf' for RandomForest. The\n",
    "                      options are as follows:\n",
    "                            - Classifiers (clf)\n",
    "                                * 'rdf' for RandomForestClassifier (uses\n",
    "                                oob_score)\n",
    "                                * 'xgb' for  XGBClassifier\n",
    "                            - Regression (reg):\n",
    "                                * 'lin' for LinearRegression\n",
    "                                * 'rdf' for RandomForestRegressor\n",
    "                                * 'xgb' for XGBRegressor\n",
    "                                * 'svr' for Support Vector Regressor\n",
    "                                * 'ridge' for Ridge Regression\n",
    "                                * 'lasso' for Lasso Regression\n",
    "        - model_params: the parameters to give the model. Defaulted to None.\n",
    "        - cv_fold: the number of folds to use in cross validation. Defa-\n",
    "                   ulted to 2.\n",
    "        - num_trials: the number of random samples or trials to use in\n",
    "                      RandomizedSearchCV or optuna, respectively. Def-\n",
    "                      aulted to 300.\n",
    "        - n_jobs: the number of parallel jobs to run when hypertuning.\n",
    "                  Defaulted to -1 for all available cores.\n",
    "        - tuning_strategy: the algorithm to use for hyperparameter tuning.\n",
    "                           Defaulted to 'grid'. The options are as follows:\n",
    "                            * 'grid' for GridSearchCV\n",
    "                            * 'random' for RandomizedSearchCV\n",
    "                            * 'bayesian' for Bayesian optimization using\n",
    "                               TPESampler (Default of optuna)\n",
    "                            * 'auto' for Auto-sampler (optunahub)\n",
    "        - model: the model that is created and trained. If there is\n",
    "                 hypertuning, the best model is stored here.\n",
    "        - best_params: the best hyperparameters found during hypertuning.\n",
    "                       Defaulted to None if no hypertuning is done.\n",
    "        - v_MSE: the best MSE (i.e. lowest) obtained on the validation set (for\n",
    "                 regression only)\n",
    "        - v_r2: the best r-squared score obtained on the validation set (for\n",
    "                regression only)\n",
    "        - t_MSE: the best MSE (i.e. lowest) obtained during the training phase (for\n",
    "                 regression only)\n",
    "        - t_r2: the best r-squared score obtained during the training phase (for\n",
    "                regression only)\n",
    "\n",
    "    Methods:\n",
    "        - __init__(): the constructor for the class\n",
    "        - make_full_model(): makes, trains, and hypertunes  a model on various\n",
    "                             available hyperparameters. It uses the hidden\n",
    "                             attributes to control model specifications and\n",
    "                             doesn't require the user to pass in parameters to\n",
    "                             access full hyperparameters.\n",
    "        - save_model(): saves the made model to a joblib file.\n",
    "\n",
    "    Hidden Attributes:\n",
    "\n",
    "        - RandomForest:\n",
    "            * _rdf_nestimators_range: the range of n_estimators to use.\n",
    "                                      Defaulted to (100, 301)\n",
    "            * _rdf_nestimators_step: the step size for n_estimators.\n",
    "                                      Defaulted to 1\n",
    "            * _rdf_maxdep_range: the range of max_depth to use. Defaulted\n",
    "                                 to (4, 15)\n",
    "            * _rdf_maxdep_step: the step size for max_depth. Defaulted\n",
    "                                to 2\n",
    "            * _rdf_minleaf_range: the range of min_samples_leaf to use.\n",
    "                                  Defaulted to (2, 7)\n",
    "            * _rdf_minleaf_step: the step size for min_samples_leaf.\n",
    "                                 Defaulted to 1\n",
    "            * _rdf_minsamples_range: the range of min_samples_split to use.\n",
    "                                     Defaulted to (2, 7)\n",
    "            * _rdf_minsamples_step: the step size for min_samples_split.\n",
    "                                    Defaulted to 1\n",
    "            * _rdf_maxfeat_range: the range of max_features to use.\n",
    "                                  Defaulted to (4, 15)\n",
    "            * _rdf_maxfeat_step: the step size for max_features. Defaulted\n",
    "                                 to 1\n",
    "            * _rdf_criterion_reg: the criterion to use for RandomForestRegressor.\n",
    "                                  Defaulted to \"mse\"  (mean squared error)\n",
    "\n",
    "        - XGBoost:\n",
    "            * _xgb_objective_clf: the objective for XGBoost classifier.\n",
    "                                  Defaulted to \"multi:softmax\"\n",
    "            * _xgb_num_classes_clf: the number of classes/labels for\n",
    "                                    XGBoost classifier. Defaulted to 9\n",
    "            * _xgb_nestimators_range: the range of n_estimators to use.\n",
    "                                      Defaulted to (100, 301)\n",
    "            * _xgb_nestimators_step: the step size for n_estimators.\n",
    "                                     Defaulted to 1\n",
    "            * _gxb_eta_range: the range of eta/learning rate to use.\n",
    "                              Defaulted to (0.001, 0.1)\n",
    "            * _alpha_range: the range of alpha to use (L1 regularization).\n",
    "                            Defaulted to (0.6, 10.1)\n",
    "            * _lambda_range: the range of lambda to use (L2 regularization).\n",
    "                             Defaulted to (0.6, 10.1)\n",
    "            * _gamma_range: the range of gamma to use (minimum loss reduction\n",
    "                            or penalty for many leaves). Defaulted to (0.6, 10.1)\n",
    "            * _xgb_max_depth_range: the range of max_depth to use. Defaulted\n",
    "                                    to (3, 10)\n",
    "            * _xgb_max_depth_step: the step size for max_depth. Defaulted to 1\n",
    "            * _xgb_objective_reg: the objective for XGBoost regressor. Defaulted\n",
    "                                  to \"reg:squarederror\"\n",
    "\n",
    "        - Support Vector Regressor (SVR):\n",
    "            * _svr_kernel: the kernel to use for SVR. Defaulted to \"rbf\". Options\n",
    "                            are as follows (from sklearn):\n",
    "                                * 'linear' for linear kernel\n",
    "                                * 'poly' for polynomial kernel\n",
    "                                * 'rbf' for radial basis function kernel\n",
    "                                * 'sigmoid' for sigmoid kernel\n",
    "            * _svr_c_range: the range of C to use for SVR. Defaulted to (0.1, 10.5).\n",
    "                            Note that strength of regularization is inversely prop-\n",
    "                            ortional to C.\n",
    "            * _svr_epsilon_range: the range of epsilon to use for SVR. Defaulted to\n",
    "                                 (0.01, 1.0)\n",
    "            * _svr_gamma: the gamma parameter for the kernel. Defaulted to \"scale\"\n",
    "            * _svr_poly_degree_range: the range of polynomial degrees to use for\n",
    "                                     SVR. Defaulted to (2, 5).\n",
    "            * _svr_poly_degree_step: the step size for polynomial degrees. Defaulted\n",
    "                                     to 1\n",
    "            * _svr_coef0_range: the range of coef0 to use for SVR. Defaulted to (0.0, 5.0)\n",
    "        \n",
    "        - Ridge:\n",
    "            * _ridge_alpha_range: a tuple for the range to find the best regularization constant. \n",
    "                                  Defaulted to (0.1, 10.5)\n",
    "            * _ridge_max_iters: an int to specify the max number of iterations to use for the \n",
    "                                solver algorithm. Defaulted to 8000\n",
    "\n",
    "        # Hidden attributes for Lasso Regression\n",
    "        s._lasso_alpha_range = (0.1, 10.5)\n",
    "        self._lasso_max_iters = 8000\n",
    "        - Lasso:\n",
    "            * _lasso_alpha_range: a tuple for the range to find the best regularization constant.\n",
    "                                  Defaulted to (0.1, 10.5)\n",
    "            * _lasso_max_iters: an int to specify the max number of iterations to use for convergence.\n",
    "                                Defaulted to 8000\n",
    "\n",
    "    Hidden Methods:\n",
    "        - _rdf_obj(): a hidden method used to train and hypertune a RandomForest\n",
    "                      model using optuna.\n",
    "        - _get_rdf(): a hidden method used to get the best RandomForest model\n",
    "                      after hypertuning.\n",
    "        - _xgb_obj(): a hidden method used to train and hypertune a XGBoost\n",
    "                      model using optuna.\n",
    "        - _get_xgb(): a hidden method used to get the best XGBoost model after\n",
    "                      hypertuning.\n",
    "        - _svr_obj(): a hidden method used to train and hypertune a Support\n",
    "                      Vector Regressor using optuna.\n",
    "        - _get_svr(): a hidden method used to get the best Support Vector\n",
    "                      Regressor model after hypertuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_choice: str = \"reg\", est_type: str = \"rdf\", params: Dict = None, cv_fold: int = 2,\n",
    "                 tuning_strategy: str = \"grid\", num_trials: int = 300, n_jobs: int = -1, **kwargs) -> None:\n",
    "        \"\"\"This function defines a user defined supervised learning model and estimator\n",
    "        according to the given input\n",
    "\n",
    "        Parameters:\n",
    "            - model_choice (str): the choice of model to make. Defaulted to 'reg'.\n",
    "            - est_type (str): the type of estimator to use for a given model choice.\n",
    "                              Defaulted to 'rdf' for RandomForest.\n",
    "            - params (Dict): the parameters to give the model. Defaulted to None.\n",
    "            - cv_fold (int): the number of folds to use when cross validating.\n",
    "                             Defaulted to 2 for 2-fold cross validation.\n",
    "            - tuning_strategy: the algorithm to use for hyperparameter tuning.\n",
    "                               Defaulted to 'grid' for GridSearchCV.\n",
    "            - num_trials (int): number of samples to use when performing baye-\n",
    "                                 sian optimization or randomized search.\n",
    "            - n_jobs: the number of parallel jobs to run when hypertuning.\n",
    "                      Defaulted to -1 for all available cores.\n",
    "            - kwargs: keyword arguments meant for regression class\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Check user input\n",
    "        if (model_choice.strip().lower() != \"clf\") and (model_choice.strip().lower() != \"reg\"):\n",
    "            raise ValueError(f\"Model type must be either 'clf' for classification or 'reg' for regression. Got {model_choice}\")\n",
    "        if (est_type.strip().lower() != 'rdf') and (est_type.strip().lower() != 'xgb') and (\n",
    "            est_type.strip().lower() != 'lin') and (est_type.strip().lower() != 'svr') and (\n",
    "            est_type.strip().lower() != 'ridge') and (est_type.strip().lower() != 'lasso'):\n",
    "            raise ValueError(\n",
    "                \"Model type is not found. Please refer to the documentation to choose and appropriate model.\")\n",
    "        if (cv_fold is None) or (cv_fold < 2):\n",
    "            raise TypeError(\"cv_fold must be of type int that is greater than or equal to 2.\")\n",
    "        if (tuning_strategy != \"grid\") and (tuning_strategy != \"random\") and (\n",
    "            tuning_strategy != \"bayesian\") and (tuning_strategy != \"auto\"):\n",
    "            raise TypeError(\"Hyperparameter tuning strategy must be either 'auto', 'bayesian', 'grid', or 'random'.\")\n",
    "        if not isinstance(num_trials, int):\n",
    "            raise TypeError(\"num_trials must be of type int\")\n",
    "        if not isinstance(n_jobs, int):\n",
    "            raise TypeError(\"n_jobs must be of type int\")\n",
    "\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        # Define the attributes\n",
    "        self.model_choice = model_choice.strip().lower()\n",
    "        self.est_type = est_type.strip().lower()\n",
    "        self.model_params = params\n",
    "        self.cv_fold = cv_fold\n",
    "        self.tuning_strategy = tuning_strategy.strip().lower()\n",
    "        self.num_trials = num_trials\n",
    "        self.n_jobs = n_jobs\n",
    "        self.model = None\n",
    "\n",
    "        # Hidden attributes for random forest (Hyperparameter tunining)\n",
    "        self._rdf_nestimators_range = (25, 75)  # Uses np.arange (so go one above)\n",
    "        self._rdf_nestimators_step = 1\n",
    "        self._rdf_maxdep_range = (5, 15)\n",
    "        self._rdf_maxdep_step = 2\n",
    "        self._rdf_minleaf_range = (3, 30)\n",
    "        self._rdf_minleaf_step = 1\n",
    "        self._rdf_minsamples_range = (2, 7)\n",
    "        self._rdf_minsamples_step = 1\n",
    "        self._rdf_maxfeat_range = (4, 15)\n",
    "        self._rdf_maxfeat_step = 1\n",
    "        self._rdf_criterion_reg = \"squared_error\"\n",
    "\n",
    "        # Hidden attributes for xgboost\n",
    "        self._xgb_objective_clf = \"multi:softmax\"\n",
    "        self._xgb_num_classes_clf = 9\n",
    "        self._xgb_nestimators_range = (25, 90)\n",
    "        self._xgb_nestimators_step = 1\n",
    "        self._gxb_eta_range = (0.001, 0.01)\n",
    "        self._alpha_range = (0.6, 10.1)\n",
    "        self._lambda_range = (0.6, 10.1)\n",
    "        self._gamma_range = (0.6, 10.1)\n",
    "        self._xgb_max_depth_range = (4, 15)\n",
    "        self._xgb_max_depth_step = 1\n",
    "        self._xgb_objective_reg = \"reg:squarederror\"\n",
    "\n",
    "        # Hidden attributes for Support Vector Regressor (SVR)\n",
    "        self._svr_kernel = \"rbf\"\n",
    "        self._svr_c_range = (0.1, 10.5)\n",
    "        self._svr_epsilon_range = (0.01, 1.0)\n",
    "        self._svr_gamma_range = (0.1, 2.5)      # For rbf, poly, and sigmoid kernels\n",
    "        self._svr_poly_degree_range = (2, 5)    # Poly kernel parameters\n",
    "        self._svr_poly_degree_step = 1\n",
    "        self._svr_coef0_range = (0.1, 5.0)      # For poly, rbf, and sigmoid kernels\n",
    "\n",
    "        # Hidden attributes for Ridge Regression\n",
    "        self._ridge_alpha_range = (0.1, 10.5)\n",
    "        self._ridge_max_iters = 8000\n",
    "\n",
    "        # Hidden attributes for Lasso Regression\n",
    "        self._lasso_alpha_range = (0.1, 10.5)\n",
    "        self._lasso_max_iters = 8000\n",
    "\n",
    "    # Hidden methods for optuna hyperparameter tuning\n",
    "    def _rdf_obj(self, trial:optuna.Trial, X_train:NDArray, y_train:ArrayLike) -> float | Tuple[float|float]:\n",
    "        \"\"\"This function accepts a trial object and creates and trains a Random\n",
    "        ForestClassifier with the specified hyperparameters as given by optuna\n",
    "        using an optimization algorithm (be it TPESampler or Autosampler).\n",
    "\n",
    "        Parameters:\n",
    "            - trial (optuna.Trial): a specific trial object meant to signify the\n",
    "                                    current trial/model optuna is training\n",
    "            - X_train (NDArray): the training data\n",
    "            - y_train (ArrayLike): the target data\n",
    "\n",
    "        Returns:\n",
    "            - (float): the oob_score for the classification algorithm\n",
    "            OR \n",
    "            - (float): the MSE on the oob-samples predictions (i.e. validation set)\n",
    "            - (float): the oob_score (r^2 as per sklearn docs) on the oob-samples\n",
    "                       prediction\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is None:\n",
    "            params = {\"n_estimators\": trial.suggest_int(\"n_estimators\", low=self._rdf_nestimators_range[0], high=self._rdf_nestimators_range[1], step=self._rdf_nestimators_step),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", low=self._rdf_maxdep_range[0], high=self._rdf_maxdep_range[1], step=self._rdf_maxdep_step),\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", low=self._rdf_minleaf_range[0], high=self._rdf_minleaf_range[1], step=self._rdf_minleaf_step),\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", low=self._rdf_minsamples_range[0], high=self._rdf_minsamples_range[1], step=self._rdf_minsamples_step),\n",
    "                    \"max_features\": trial.suggest_int(\"max_features\", low=self._rdf_maxfeat_range[0],high=self._rdf_minsamples_range[1], step=self._rdf_maxfeat_step)\n",
    "                    }\n",
    "        else:\n",
    "            params = self.model_params\n",
    "\n",
    "        # Make the model\n",
    "        if self.model_choice == \"clf\":\n",
    "            params[\"criterion\"] = trial.suggest_categorical(\"criterion\", [\"gini\", \"cross_entropy\", \"log_loss\"]),\n",
    "            model = RandomForestClassifier(**params, oob_score=True)\n",
    "            scores = cross_val_score(model, X_train, y_train, scoring=lambda est, X, y: est.oob_score_,\n",
    "                                    n_jobs=self.n_jobs,\n",
    "                                    cv=self.cv_fold)\n",
    "\n",
    "            return scores.mean()\n",
    "\n",
    "        # Regressor\n",
    "        else:\n",
    "            params[\"criterion\"] = self._rdf_criterion_reg\n",
    "            model = RandomForestRegressor(**params, oob_score=True, n_jobs=self.n_jobs)\n",
    "            #oob_scorer = lambda est, X, y: est.oob_score_\n",
    "            #scoring = {\"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "            #          \"oob\": oob_scorer}                                                        # Make a metric out of the oob_score for CV\n",
    "            #scores = cross_validate(estimator=model, X=X_train, y=y_train, scoring=scoring, cv=self.cv_fold, n_jobs=self.n_jobs, return_train_score=True)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            mse = mean_squared_error(y_true=y_train, y_pred=model.oob_prediction_)               # MSE on validation (oob)\n",
    "            #scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error')\n",
    "            #oobs = (scores['train_oob']).mean()\n",
    "            #mses = -((scores['test_neg_mean_squared_error']).mean())\n",
    "        \n",
    "            return mse, model.oob_score_ #mses, oobs #-scores.mean()\n",
    "\n",
    "    def _get_rdf(self, trial: optuna.Trial, X_train: NDArray,\n",
    "                 y_train: ArrayLike) -> RandomForestClassifier | RandomForestRegressor:\n",
    "        \"\"\"This is a helper function meant to accept the best optuna trial\n",
    "        and return the best RandomForest model.\n",
    "\n",
    "        Parameters:\n",
    "            - trial (optuna.Trial): the best trial object from optuna\n",
    "            - X_train (ArrayLike): the training data\n",
    "            - y_train (ArrayLike): the target data\n",
    "\n",
    "        Returns:\n",
    "            - (RandomForestClassifier): the best RandomForestClassifier model\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\"n_estimators\": trial.suggest_int(\"n_estimators\", low=self._rdf_nestimators_range[0], high=self._rdf_nestimators_range[1], step=self._rdf_nestimators_step),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", low=self._rdf_maxdep_range[0], high=self._rdf_maxdep_range[1], step=self._rdf_maxdep_step),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", low=self._rdf_minleaf_range[0], high=self._rdf_minleaf_range[1], step=self._rdf_minleaf_step),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", low=self._rdf_minsamples_range[0], high=self._rdf_minsamples_range[1], step=self._rdf_minsamples_step),\n",
    "                \"max_features\": trial.suggest_int(\"max_features\", low=self._rdf_maxfeat_range[0],high=self._rdf_minsamples_range[1], step=self._rdf_maxfeat_step)\n",
    "                }\n",
    "\n",
    "        # Make the model\n",
    "        if self.model_choice == \"clf\":\n",
    "            params[\"criterion\"] = trial.suggest_categorical(\"criterion\", [\"gini\", \"cross_entropy\", \"log_loss\"]),\n",
    "            model = RandomForestClassifier(**params, oob_score=True)\n",
    "        else:\n",
    "            params[\"criterion\"] = self._rdf_criterion_reg\n",
    "            model = RandomForestRegressor(**params, oob_score=True)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _xgb_obj(self, trial: optuna.trial, X_train, y_train) -> float|Tuple[float, float]:\n",
    "        \"\"\"This function accepts an optuna trial module that indicates the\n",
    "        current trial of hyperparameter tuning and creates a XGBoost model to\n",
    "        train. It returns the score after having trained the classifier or\n",
    "        regressor. This function serves as a single call during each\n",
    "        trial by the study object. The trial uses the TPESampler Bayesian\n",
    "        or Autosampler optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            - trial (optuna.trial): the current trial of hyperparameter tuning\n",
    "            - X_train (ArrayLike): the training data\n",
    "            - y_train (ArrayLike): the target data\n",
    "\n",
    "        Returns:\n",
    "            - (float): the classifier score on the testing dataset\n",
    "            OR\n",
    "            - (float): the MSE on the validation set predictions \n",
    "            - (float): the R^@ score on the validation set predictions \n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is None:\n",
    "            params = {\"n_estimators\": trial.suggest_int(\"n_estimators\", low=self._xgb_nestimators_range[0], high=self._xgb_nestimators_range[1], step=self._xgb_nestimators_step),\n",
    "                      \"eta\": trial.suggest_float(\"eta\", low=self._gxb_eta_range[0], high=self._gxb_eta_range[1], log=True),\n",
    "                      \"alpha\": trial.suggest_float(\"alpha\", low=self._alpha_range[0], high=self._alpha_range[1]),\n",
    "                      \"lambda\": trial.suggest_float(\"lambda\", low=self._lambda_range[0], high=self._lambda_range[1]),\n",
    "                      \"gamma\": trial.suggest_float(\"gamma\", low=self._gamma_range[0], high=self._gamma_range[1]),\n",
    "                      \"max_depth\": trial.suggest_int(\"max_depth\", low=self._xgb_max_depth_range[0], high=self._xgb_max_depth_range[1], step=self._xgb_max_depth_step),\n",
    "                      }\n",
    "        else:\n",
    "            params = self.model_params\n",
    "\n",
    "        # Make the model\n",
    "        if self.model_choice == \"clf\":\n",
    "            params[\"objective\"] = self._xgb_objective_clf\n",
    "            params[\"num_classes\"] = self._xgb_num_classes_clf\n",
    "            model = XGBClassifier(**params)\n",
    "            scores = cross_val_score(model, X_train, y_train, n_jobs=self.n_jobs, cv=self.cv_fold)\n",
    "            mean = scores.mean()\n",
    "\n",
    "            return mean\n",
    "\n",
    "        else:\n",
    "            params[\"objective\"] = self._xgb_objective_reg\n",
    "            model = XGBRegressor(**params)\n",
    "            scoring = ('r2', 'neg_mean_squared_error')\n",
    "            scores = cross_validate(estimator=model, X=X_train, y=y_train, n_jobs=self.n_jobs, cv=self.cv_fold, scoring=scoring)\n",
    "            #scores = cross_val_score(model, X_train, y_train, n_jobs=self.n_jobs, cv=self.cv_fold, scoring='neg_mean_squared_error')\n",
    "            mse = -(scores['test_neg_mean_squared_error'].mean())\n",
    "            r2 = scores['test_r2'].mean()\n",
    "\n",
    "            return mse, r2\n",
    "\n",
    "    def _get_xgb(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> XGBClassifier | XGBRegressor:\n",
    "        \"\"\"This function accepts the best trial from optuna and returns the\n",
    "        best XGBoost model according to the given hyperparameters.\n",
    "\n",
    "        Parameters:\n",
    "        - trial (optuna.Trial): the best trial from optuna\n",
    "        - X_train (ArrayLike): the training data\n",
    "        - y_train (ArrayLike): the target data\n",
    "\n",
    "        Returns:\n",
    "        - (XGBClassifier or XGBRegressor): the best XGBoost model\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        params = {\"n_estimators\": trial.suggest_int(\"n_estimators\", low=self._xgb_nestimators_range[0], high=self._xgb_nestimators_range[1], step=self._xgb_nestimators_step),\n",
    "                    \"eta\": trial.suggest_float(\"eta\", low=self._gxb_eta_range[0], high=self._gxb_eta_range[1], log=True),\n",
    "                    \"alpha\": trial.suggest_float(\"alpha\", low=self._alpha_range[0], high=self._alpha_range[1]),\n",
    "                    \"lambda\": trial.suggest_float(\"lambda\", low=self._lambda_range[0], high=self._lambda_range[1]),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", low=self._gamma_range[0], high=self._gamma_range[1]),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", low=self._xgb_max_depth_range[0], high=self._xgb_max_depth_range[1], step=self._xgb_max_depth_step),\n",
    "                    }\n",
    "\n",
    "        # Make the model\n",
    "        if self.model_choice == \"clf\":\n",
    "            params[\"objective\"] = self._xgb_objective_clf\n",
    "            params[\"num_classes\"] = self._xgb_num_classes_clf\n",
    "            model = XGBClassifier(**params)\n",
    "        else:\n",
    "            params[\"objective\"] = self._xgb_objective_reg\n",
    "            model = XGBRegressor(**params)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _svr_obj(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> float:\n",
    "        \"\"\"This function accepts a trial object and creates and trains a Support Vector\n",
    "        Regressor with the specified hyperparameters as given by optuna using TPESampler\n",
    "        or Autosampler from optuna.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is None:\n",
    "            params = {\n",
    "                      \"C\": trial.suggest_float(\"C\", low=self._svr_c_range[0], high=self._svr_c_range[1]),\n",
    "                      \"epsilon\": trial.suggest_float(\"epsilon\", low=self._svr_epsilon_range[0], high=self._svr_epsilon_range[1])\n",
    "                      }\n",
    "            params[\"kernel\"] = self._svr_kernel\n",
    "\n",
    "            if self._svr_kernel == \"poly\":\n",
    "                params[\"degree\"] = trial.suggest_int(\"degree\", low=self._svr_poly_degree_range[0], high=self._svr_poly_degree_range[1], step=self._svr_poly_degree_step)\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "                params[\"coef0\"] = trial.suggest_float(\"coef0\", low=self._svr_coef0_range[0], high=self._svr_coef0_range[1])\n",
    "\n",
    "            elif self._svr_kernel == \"sigmoid\":\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "                params[\"coef0\"] = trial.suggest_float(\"coef0\", low=self._svr_coef0_range[0], high=self._svr_coef0_range[1])\n",
    "\n",
    "            elif self._svr_kernel == \"rbf\":\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "\n",
    "        else:\n",
    "            params = self.model_params\n",
    "\n",
    "        # Make the model\n",
    "        model = SVR(**params)\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=self.n_jobs,\n",
    "                                 cv=self.cv_fold)\n",
    "\n",
    "        return -scores.mean()\n",
    "\n",
    "    def _get_svr(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> SVR:\n",
    "        \"\"\"This is a helper function meant to accept the best optuna trial\n",
    "        and return the best Support Vector Regressor model.\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "                  \"C\": trial.suggest_float(\"C\", low=self._svr_c_range[0], high=self._svr_c_range[1]),\n",
    "                  \"epsilon\": trial.suggest_float(\"epsilon\", low=self._svr_epsilon_range[0], high=self._svr_epsilon_range[1])\n",
    "                      }\n",
    "        params[\"kernel\"] = self._svr_kernel\n",
    "\n",
    "        if self._svr_kernel == \"poly\":\n",
    "                params[\"degree\"] = trial.suggest_int(\"degree\", low=self._svr_poly_degree_range[0], high=self._svr_poly_degree_range[1], step=self._svr_poly_degree_step)\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "                params[\"coef0\"] = trial.suggest_float(\"coef0\", low=self._svr_coef0_range[0], high=self._svr_coef0_range[1])\n",
    "\n",
    "        elif self._svr_kernel == \"sigmoid\":\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "                params[\"coef0\"] = trial.suggest_float(\"coef0\", low=self._svr_coef0_range[0], high=self._svr_coef0_range[1])\n",
    "\n",
    "        elif self._svr_kernel == \"rbf\":\n",
    "                params[\"gamma\"] = trial.suggest_float(\"gamma\", low=self._svr_gamma_range[0], high=self._svr_gamma_range[1])\n",
    "                \n",
    "        # Make the model\n",
    "        model = SVR(**params)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _ridge_obj(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> float:\n",
    "        \"\"\"This function accepts a trial object and creates and trains a Ridge\n",
    "        Regressor with the specified hyperparameters as given by optuna using\n",
    "        an optuna optimization algorithm (Autosampler or TPESampler).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is None:\n",
    "            params = {\"alpha\": trial.suggest_float(\"alpha\", low=self._ridge_alpha_range[0], high=self._ridge_alpha_range[1]),\n",
    "                      \"solver\": trial.suggest_categorical(\"solver\", [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"])\n",
    "                      }\n",
    "        else:\n",
    "            params = self.model_params\n",
    "\n",
    "        # Make the model\n",
    "        model = Ridge(**params, max_iter=self._ridge_max_iters)\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=self.n_jobs,\n",
    "                                 cv=self.cv_fold)\n",
    "\n",
    "        return -scores.mean()\n",
    "\n",
    "    def _get_ridge(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> Ridge:\n",
    "        \"\"\"This is a helper function meant to accept the best optuna trial\n",
    "        and return the best Ridge model.\n",
    "        \"\"\"\n",
    "        params = {\"alpha\": trial.suggest_float(\"alpha\", low=self._ridge_alpha_range[0], high=self._ridge_alpha_range[1]),\n",
    "                    \"solver\": trial.suggest_categorical(\"solver\", [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"])\n",
    "                    }\n",
    "\n",
    "        # Make the model\n",
    "        model = Ridge(**params, max_iter=self._ridge_max_iters)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _lasso_obj(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> float:\n",
    "        \"\"\"This function accepts a trial object and creates and trains a Lasso\n",
    "        Regressor with the specified hyperparameters as given by optuna using\n",
    "        an optuna optimization algorithm (Autosampler or TPESampler).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is None:\n",
    "            params = {\"alpha\": trial.suggest_float(\"alpha\", low=self._lasso_alpha_range[0], high=self._lasso_alpha_range[1]),\n",
    "                      }\n",
    "        else:\n",
    "            params = self.model_params\n",
    "\n",
    "        # Make the model\n",
    "        model = Lasso(**params, max_iter=self._lasso_max_iters)\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=self.n_jobs,\n",
    "                                 cv=self.cv_fold)\n",
    "\n",
    "        return -scores.mean()\n",
    "\n",
    "    def _get_lasso(self, trial: optuna.Trial, X_train: NDArray, y_train: ArrayLike) -> Lasso:\n",
    "        \"\"\"This is a helper function meant to accept the best optuna trial\n",
    "        and return the best Lasso model.\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\"alpha\": trial.suggest_float(\"alpha\", low=self._lasso_alpha_range[0], high=self._lasso_alpha_range[1]),\n",
    "                    }\n",
    "\n",
    "        # Make the model\n",
    "        model = Lasso(**params, max_iter=self._lasso_max_iters)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def make_full_model(self, X_train: NDArray, y_train: ArrayLike) -> None:\n",
    "        \"\"\"This methods makes a model that trains and hypertunes on\n",
    "        all available hyperparameters. It uses the hidden attributes\n",
    "        to control model specifications and doesn't require the user\n",
    "        to pass in parameters to access full hyperparameters. It\n",
    "        stores the model as an attribute.\n",
    "\n",
    "        Parameters:\n",
    "            - X_train (ArrayLike): the training data\n",
    "            - y_train (ArrayLike): the target data\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model_params is not None:\n",
    "            raise ValueError(\"Cannot make a model that trains and hypertunes on all available hyperparameters \" + \\\n",
    "                             \"when given user defined parameters. Use the hidden attributes to control model \" + \\\n",
    "                             \"specifications and don't pass in parameters to access full hyperparameters.\")\n",
    "\n",
    "        warnings.warn(message=f\"NOTE: Please check the hidden attributes for the model choice: {self.model_choice}, and estimator: {self.est_type} before hypertuning \" +\\\n",
    "                               \"should you want to have different hyperparameters than the ones set as default.\")\n",
    "        time.sleep(3)\n",
    "        print(\"Now continuing.\")\n",
    "        \n",
    "        # Make grid search\n",
    "        if self.tuning_strategy == \"grid\":\n",
    "\n",
    "            # Make and hypertune classification models\n",
    "            if self.model_choice == \"clf\":\n",
    "\n",
    "                if self.est_type == \"rdf\":\n",
    "\n",
    "                    print(\"Now making RandomForestClassifier...\")\n",
    "                    clf = RandomForestClassifier(oob_score=True)\n",
    "                    parameters = {\"n_estimators\": [int(x) for x in np.arange(*self._rdf_nestimators_range,\n",
    "                                                                             step=self._rdf_nestimators_step)],\n",
    "                                  \"criterion\": [\"gini\", \"cross_entropy\", \"log_loss\"],\n",
    "                                  \"max_depth\": [int(x) for x in\n",
    "                                                np.arange(*self._rdf_maxdep_range, step=self._rdf_maxdep_step)],\n",
    "                                  \"min_samples_leaf\": [int(x) for x in np.arange(*self._rdf_minleaf_range,\n",
    "                                                                                 step=self._rdf_minleaf_step)],\n",
    "                                  \"min_samples_split\": [int(x) for x in np.arange(*self._rdf_minsamples_range,\n",
    "                                                                                  step=self._rdf_minsamples_step)]\n",
    "                                  }\n",
    "                    rdf_grid = GridSearchCV(estimator=clf, param_grid=parameters, n_jobs=self.n_jobs, cv=self.cv_fold,\n",
    "                                            scoring=lambda est, X, y: est.oob_score_)\n",
    "\n",
    "                    print(\"Training and hypertuning using Exhaustive Search...\")\n",
    "                    rdf_grid.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {rdf_grid.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = rdf_grid.best_estimator_\n",
    "                    self.best_params = rdf_grid.best_params_\n",
    "\n",
    "                else:\n",
    "                    print(\"Now making XGBClassifier...\")\n",
    "                    params = {\"n_estimators\": [int(x) for x in\n",
    "                                               np.arange(*self._xgb_nestimators_range, self._xgb_nestimators_step)],\n",
    "                              \"eta\": [float(x) for x in np.linspace(*self._gxb_eta_range, num=100)],\n",
    "                              \"alpha\": [float(x) for x in np.linspace(*self._alpha_range, num=100)],\n",
    "                              \"lambda\": [float(x) for x in np.linspace(*self._lambda_range, num=100)],\n",
    "                              \"gamma\": [float(x) for x in np.linspace(*self._gamma_range, num=100)],\n",
    "                              \"max_depth\": [int(x) for x in\n",
    "                                            np.arange(*self._xgb_max_depth_range, self._xgb_max_depth_step)],\n",
    "                              \"objective\": self._xgb_objective_clf,\n",
    "                              \"num_classes\": self._xgb_num_classes_clf\n",
    "                              }\n",
    "                    xgb_grid = GridSearchCV(estimator=XGBClassifier(), param_grid=params, n_jobs=self.n_jobs,\n",
    "                                            cv=self.cv_fold)\n",
    "\n",
    "                    print(\"Training and hypertuning using Exhaustive Search...\")\n",
    "                    xgb_grid.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {xgb_grid.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = xgb_grid.best_estimator_\n",
    "                    self.best_params = xgb_grid.best_params_\n",
    "\n",
    "            # Make and hypertune regression models\n",
    "            else:\n",
    "                if self.est_type == \"rdf\":\n",
    "                    print(\"Now making RandomForestRegressor...\")\n",
    "                    reg = RandomForestRegressor(oob_score=True)\n",
    "                    parameters = {\"n_estimators\": [int(x) for x in np.arange(*self._rdf_nestimators_range,\n",
    "                                                                             step=self._rdf_nestimators_step)],\n",
    "                                  \"criterion\": [\"squared_error\"],\n",
    "                                  \"max_depth\": [int(x) for x in\n",
    "                                                np.arange(*self._rdf_maxdep_range, step=self._rdf_maxdep_step)],\n",
    "                                  \"min_samples_leaf\": [int(x) for x in np.arange(*self._rdf_minleaf_range,\n",
    "                                                                                 step=self._rdf_minleaf_step)],\n",
    "                                  \"min_samples_split\": [int(x) for x in np.arange(*self._rdf_minsamples_range,\n",
    "                                                                                  step=self._rdf_minsamples_step)],\n",
    "                                  \"max_features\": [int(x) for x in\n",
    "                                                   np.arange(*self._rdf_maxfeat_range, step=self._rdf_maxfeat_step)],\n",
    "                                  }\n",
    "                    rdf_grid = GridSearchCV(estimator=reg, param_grid=parameters, n_jobs=self.n_jobs, cv=self.cv_fold,\n",
    "                                            scoring='neg_mean_squared_error')\n",
    "\n",
    "                    print(\"Training and hypertuning using Exhaustive Search...\")\n",
    "                    rdf_grid.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {rdf_grid.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = rdf_grid.best_estimator_\n",
    "                    self.best_params = rdf_grid.best_params_\n",
    "\n",
    "                elif self.est_type == \"xgb\":\n",
    "                    print(\"Now making XGBRegressor...\")\n",
    "                    params = {\"n_estimators\": [int(x) for x in\n",
    "                                               np.arange(*self._xgb_nestimators_range, self._xgb_nestimators_step)],\n",
    "                              \"eta\": [float(x) for x in np.linspace(*self._gxb_eta_range, num=100)],\n",
    "                              \"alpha\": [float(x) for x in np.linspace(*self._alpha_range, num=100)],\n",
    "                              \"lambda\": [float(x) for x in np.linspace(*self._lambda_range, num=100)],\n",
    "                              \"gamma\": [float(x) for x in np.linspace(*self._gamma_range, num=100)],\n",
    "                              \"max_depth\": [int(x) for x in\n",
    "                                            np.arange(*self._xgb_max_depth_range, self._xgb_max_depth_step)],\n",
    "                              \"objective\": self._xgb_objective_reg,\n",
    "                              }\n",
    "                    xgb_grid = GridSearchCV(estimator=XGBRegressor(), param_grid=params, n_jobs=self.n_jobs,\n",
    "                                            cv=self.cv_fold)\n",
    "\n",
    "                    print(\"Training and hypertuning using Exhaustive Search...\")\n",
    "                    xgb_grid.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {xgb_grid.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = xgb_grid.best_estimator_\n",
    "                    self.best_params = xgb_grid.best_params_\n",
    "\n",
    "                else:\n",
    "                    print(\"Making making LinearRegression\")\n",
    "                    lin = LinearRegression(n_jobs=-1)\n",
    "                    print(\"Training...\")\n",
    "                    lin.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "                    print(\"Trained model saved as an attribute to class\")\n",
    "                    self.model = lin\n",
    "                    self.best_params = None\n",
    "\n",
    "        # Train and tune using randomized search\n",
    "        elif self.tuning_strategy == \"random\":\n",
    "\n",
    "            # Make and hypertune classification models\n",
    "            if self.model_choice == \"clf\":\n",
    "\n",
    "                if self.est_type == \"rdf\":\n",
    "                    print(\"Now making RandomForestClassifier...\")\n",
    "                    clf = RandomForestClassifier(oob_score=True)\n",
    "                    distribs = {\"n_estimators\": [int(x) for x in np.arange(*self._rdf_nestimators_range,\n",
    "                                                                           step=self._rdf_nestimators_step)],\n",
    "                                \"criterion\": [\"gini\", \"cross_entropy\", \"log_loss\"],\n",
    "                                \"max_depth\": [int(x) for x in\n",
    "                                              np.arange(*self._rdf_maxdep_range, step=self._rdf_maxdep_step)],\n",
    "                                \"min_samples_leaf\": [int(x) for x in\n",
    "                                                     np.arange(*self._rdf_minleaf_range, step=self._rdf_minleaf_step)],\n",
    "                                \"min_samples_split\": [int(x) for x in np.arange(*self._rdf_minsamples_range,\n",
    "                                                                                step=self._rdf_minsamples_step)],\n",
    "                                \"max_features\": [int(x) for x in\n",
    "                                                 np.arange(*self._rdf_maxfeat_range, step=self._rdf_maxfeat_step)],\n",
    "                                }\n",
    "                    rdf_rand = RandomizedSearchCV(estimator=clf, param_distributions=distribs, n_jobs=self.n_jobs,\n",
    "                                                  cv=self.cv_fold,\n",
    "                                                  n_iter=self.num_trials, scoring=lambda est, X, y: est.oob_score_)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    rdf_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {rdf_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = rdf_rand.best_estimator_\n",
    "                    self.best_params = rdf_rand.best_params_\n",
    "\n",
    "                else:\n",
    "                    print(\"Now making XGBClassifier...\")\n",
    "                    params = {\"n_estimators\": [int(x) for x in\n",
    "                                               np.arange(*self._xgb_nestimators_range, self._xgb_nestimators_step)],\n",
    "                              \"eta\": [float(x) for x in np.linspace(*self._gxb_eta_range, num=100)],\n",
    "                              \"alpha\": [float(x) for x in np.linspace(*self._alpha_range, num=100)],\n",
    "                              \"lambda\": [float(x) for x in np.linspace(*self._lambda_range, num=100)],\n",
    "                              \"gamma\": [float(x) for x in np.linspace(*self._gamma_range, num=100)],\n",
    "                              \"max_depth\": [int(x) for x in\n",
    "                                            np.arange(*self._xgb_max_depth_range, self._xgb_max_depth_step)],\n",
    "                              \"objective\": self._xgb_objective_clf,\n",
    "                              \"num_classes\": self._xgb_num_classes_clf\n",
    "                              }\n",
    "                    xgb_rand = RandomizedSearchCV(estimator=XGBClassifier(), param_distributions=params,\n",
    "                                                  n_jobs=self.n_jobs,\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    xgb_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best oob_score is {xgb_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = xgb_rand.best_estimator_\n",
    "                    self.best_params = xgb_rand.best_params_\n",
    "\n",
    "            else:\n",
    "                if self.est_type == \"rdf\":\n",
    "                    print(\"Now making RandomForestRegressor...\")\n",
    "                    reg = RandomForestRegressor(oob_score=True)\n",
    "                    parameters = {\"n_estimators\": [int(x) for x in np.arange(*self._rdf_nestimators_range,\n",
    "                                                                             step=self._rdf_nestimators_step)],\n",
    "                                  \"criterion\": [\"squared_error\"],\n",
    "                                  \"max_depth\": [int(x) for x in\n",
    "                                                np.arange(*self._rdf_maxdep_range, step=self._rdf_maxdep_step)],\n",
    "                                  \"min_samples_leaf\": [int(x) for x in np.arange(*self._rdf_minleaf_range,\n",
    "                                                                                 step=self._rdf_minleaf_step)],\n",
    "                                  \"min_samples_split\": [int(x) for x in np.arange(*self._rdf_minsamples_range,\n",
    "                                                                                  step=self._rdf_minsamples_step)],\n",
    "                                  \"max_features\": [int(x) for x in\n",
    "                                                   np.arange(*self._rdf_maxfeat_range, step=self._rdf_maxfeat_step)],\n",
    "                                  }\n",
    "                    rdf_rand = RandomizedSearchCV(estimator=reg, param_distributions=parameters, n_jobs=self.n_jobs,\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials,\n",
    "                                                  scoring='neg_mean_squared_error')\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    rdf_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best MSE is {rdf_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = rdf_rand.best_estimator_\n",
    "                    self.best_params = rdf_rand.best_params_\n",
    "\n",
    "                elif self.est_type == \"xgb\":\n",
    "                    print(\"Now making XGBRegressor...\")\n",
    "                    params = {\"n_estimators\": [int(x) for x in\n",
    "                                               np.arange(*self._xgb_nestimators_range, self._xgb_nestimators_step)],\n",
    "                              \"eta\": [float(x) for x in np.linspace(*self._gxb_eta_range, num=100)],\n",
    "                              \"alpha\": [float(x) for x in np.linspace(*self._alpha_range, num=100)],\n",
    "                              \"lambda\": [float(x) for x in np.linspace(*self._lambda_range, num=100)],\n",
    "                              \"gamma\": [float(x) for x in np.linspace(*self._gamma_range, num=100)],\n",
    "                              \"max_depth\": [int(x) for x in\n",
    "                                            np.arange(*self._xgb_max_depth_range, self._xgb_max_depth_step)],\n",
    "                              \"objective\": [self._xgb_objective_reg],\n",
    "                              }\n",
    "                    reg = XGBRegressor()\n",
    "                    xgb_rand = RandomizedSearchCV(estimator=reg, param_distributions=params,\n",
    "                                                  n_jobs=self.n_jobs,\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    xgb_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best MSE is {xgb_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = xgb_rand.best_estimator_\n",
    "                    self.best_params = xgb_rand.best_params_\n",
    "\n",
    "                elif self.est_type == 'svr':\n",
    "                    print(\"Now making SVR...\")\n",
    "                    params = {\"kernel\": [\"rbf\", \"poly\", \"linear\", \"sigmoid\"],\n",
    "                              \"degree\": [int(x) for x in np.arange(*self._svr_poly_degree_range, self._svr_poly_degree_step)],\n",
    "                              \"gamma\": [float(x) for x in np.linspace(*self._svr_gamma_range, num=100)],\n",
    "                              \"C\": [float(x) for x in np.linspace(*self._svr_c_range, num=100)],\n",
    "                              \"epsilon\": [float(x) for x in np.linspace(*self._svr_epsilon_range, num=100)]\n",
    "                              }\n",
    "                    reg = SVR()\n",
    "                    svr_rand = RandomizedSearchCV(estimator=reg, param_distributions=params,\n",
    "                                                  n_jobs=self.n_jobs, scoring='neg_mean_squared_error',\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    svr_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best MSE is {svr_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = svr_rand.best_estimator_\n",
    "                    self.best_params = svr_rand.best_params_\n",
    "\n",
    "                elif self.est_type == 'ridge':\n",
    "                    print(\"Now making Ridge...\")\n",
    "                    params = {\"alpha\": [float(x) for x in np.linspace(*self._ridge_alpha_range, num=100)],\n",
    "                              \"solver\": [\"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]\n",
    "                              }\n",
    "                    reg = Ridge()\n",
    "                    ridge_rand = RandomizedSearchCV(estimator=reg, param_distributions=params,\n",
    "                                                  n_jobs=self.n_jobs, scoring='neg_mean_squared_error',\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    ridge_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best MSE is {ridge_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = ridge_rand.best_estimator_\n",
    "                    self.best_params = ridge_rand.best_params_\n",
    "\n",
    "                elif self.est_type == 'lasso':\n",
    "                    print(\"Now making Lasso...\")\n",
    "                    params = {\"alpha\": [float(x) for x in np.linspace(*self._lasso_alpha_range, num=100)]\n",
    "                              }\n",
    "                    reg = Lasso()\n",
    "                    lasso_rand = RandomizedSearchCV(estimator=reg, param_distributions=params,\n",
    "                                                  n_jobs=self.n_jobs, scoring='neg_mean_squared_error',\n",
    "                                                  cv=self.cv_fold, n_iter=self.num_trials)\n",
    "\n",
    "                    print(\"Training and hypertuning using Randomized Search...\")\n",
    "                    lasso_rand.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    # Display and save best results\n",
    "                    print(f\"The best MSE is {lasso_rand.best_score_}\")\n",
    "                    print(\"Best model and hyperparameters added as attribute to class.\")\n",
    "                    self.model = lasso_rand.best_estimator_\n",
    "                    self.best_params = lasso_rand.best_params_\n",
    "\n",
    "                else:\n",
    "                    print(\"Making making LinearRegression\")\n",
    "                    lin = LinearRegression(n_jobs=-1)\n",
    "                    print(\"Training...\")\n",
    "                    lin.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "                    print(\"Trained model saved as an attribute to class\")\n",
    "                    self.model = lin\n",
    "                    self.best_params = None\n",
    "\n",
    "        # Train and tune using Bayesian optimization (using optuna TPESampler)\n",
    "        elif self.tuning_strategy == \"bayesian\":\n",
    "            if not optuna_available:\n",
    "                raise Exception(\n",
    "                    \"optuna module is not available. Please install in order to perform bayesian optimization.\")\n",
    "            optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "            # Make classifiers\n",
    "            if self.model_choice == \"clf\":\n",
    "                if self.est_type == \"rdf\":\n",
    "\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _rdf_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._rdf_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making RandomForestClassifier using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(direction=\"maximize\", study_name=\"randfor_clf_tuning\")\n",
    "                    print(\"Training and tuning using Bayesian optimization...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best MSE is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_rdf(study.best_trial, X_train, y_train)\n",
    "\n",
    "                # XGBoost\n",
    "                else:\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _xgb_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._xgb_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making XGBClassifier using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_clf_tuning\")\n",
    "                    print(\"Training and tuning using Bayesian optimization...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_xgb(study.best_trial, X_train, y_train)\n",
    "\n",
    "            # Make regressors\n",
    "            else:\n",
    "                if self.est_type == \"rdf\":\n",
    "\n",
    "                    print(\"Making RandomForestRegressor using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(directions=[\"minimize\", \"maximize\"], study_name=\"randfor_reg_tuning\")  # Minimize the MSE and maximize the oob_score (r^2)\n",
    "                    #study = optuna.create_study(direction=\"minimize\", study_name=\"randfor_reg_tuning\")\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: self._rdf_obj(trial, X_train, y_train), n_trials=self.num_trials, n_jobs=self.n_jobs, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "                    \n",
    "                    best_trials = study.best_trials\n",
    "                    print(f\"The number of best trials: {len(best_trials)}\")\n",
    "                    print(f\"Here is a list of the best trials of(MSE, R^2):\")\n",
    "                    for i, trial in enumerate(best_trials):\n",
    "                        print(f\"\\t*Trial {trial.number} (list index {i}):\\n\\t\\tParams: {trial.params}\\n\\t\\tValues: {trial.values}\")\n",
    "\n",
    "                    best_numb = int(input(\"Select the best trial index (i.e. the index of the list): \"))\n",
    "                    best_trial  = best_trials[best_numb]\n",
    "                    self.best_params = best_trial.params\n",
    "                    self.model = self._get_rdf(best_trial, X_train, y_train)\n",
    "                    self.v_MSE = best_trial.values[0]\n",
    "                    self.v_r2 = best_trial.values[1]\n",
    "                    self.t_MSE = mean_squared_error(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "                    self.t_r2 = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "                    # self.model = self._get_rdf(study.best_trial, X_train, y_train)\n",
    "                    # self.train_r2_score = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "                    # self.rdf_oob_mse = mean_squared_error(y_true=y_train, y_pred=self.model.oob_prediction_)\n",
    "                    # self.rdf_oob_r2 = r2_score(y_true=y_train, y_pred=self.model.oob_prediction_)\n",
    "\n",
    "                elif self.est_type == \"xgb\":\n",
    "\n",
    "                    print(\"Making XGBRegressor using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(directions=[\"minimize\", \"maximize\"], study_name=\"xgb_reg_tuning\")\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: self._xgb_obj(trial, X_train, y_train), n_trials=self.num_trials, n_jobs=self.n_jobs, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    best_trials = study.best_trials\n",
    "                    print(f\"The number of best trials: {len(best_trials)}\")\n",
    "                    print(f\"Here is a list of the best trials of(MSE, R^2):\")\n",
    "                    for i, trial in enumerate(best_trials):\n",
    "                        print(f\"\\t*Trial {trial.number} (list index {i}):\\n\\t\\tParams: {trial.params}\\n\\t\\tValues: {trial.values}\")\n",
    "\n",
    "                    best_numb = int(input(\"Select the best trial index (i.e. the index of the list): \"))\n",
    "                    best_trial  = best_trials[best_numb]\n",
    "                    self.best_params = best_trial.params\n",
    "                    self.model = self._get_xgb(best_trial, X_train, y_train)\n",
    "                    self.v_MSE = best_trial.values[0]\n",
    "                    self.v_r2 = best_trial.values[1]\n",
    "                    self.t_MSE = mean_squared_error(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "                    self.t_r2 = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "\n",
    "                elif self.est_type == \"svr\":\n",
    "\n",
    "                    print(\"Making SVR with kernel \"+self._svr_kernel+\" using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"svr_tuning\")\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: self._svr_obj(trial, X_train, y_train), n_trials=self.num_trials, n_jobs=self.n_jobs, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.train_MSE = study.best_value\n",
    "                    self.model = self._get_svr(study.best_trial, X_train, y_train)\n",
    "                    self.train_r2_score = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "\n",
    "                elif self.est_type == \"ridge\":\n",
    "\n",
    "                    print(\"Making Ridge using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"ridge_tuning\")\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: self._ridge_obj(trial, X_train, y_train), n_trials=self.num_trials, n_jobs=self.n_jobs, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.train_MSE = study.best_value\n",
    "                    self.model = self._get_ridge(study.best_trial, X_train, y_train)\n",
    "                    self.train_r2_score = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "\n",
    "                elif self.est_type == \"lasso\":\n",
    "\n",
    "                    print(\"Making Lasso using Bayesian optimization...\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"lasso_tuning\")\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: self._lasso_obj(trial, X_train, y_train), n_trials=self.num_trials, n_jobs=self.n_jobs, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.train_MSE = study.best_value\n",
    "                    self.model = self._get_lasso(study.best_trial, X_train, y_train)\n",
    "                    self.train_r2_score = r2_score(y_true=y_train, y_pred=self.model.predict(X=X_train))\n",
    "\n",
    "                else:\n",
    "                    print(\"Making making LinearRegression\")\n",
    "                    lin = LinearRegression(n_jobs=self.n_jobs)\n",
    "                    print(\"Training...\")\n",
    "                    lin.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "                    print(\"Trained model saved as an attribute to class\")\n",
    "                    self.model = lin\n",
    "                    self.train_MSE = mean_squared_error(y_true=y_train, y_pred=lin.predict(X=X_train))\n",
    "                    self.best_params = None\n",
    "                    self.train_r2_score = r2_score(y_true=y_train, y_pred=lin.predict(X=X_train))\n",
    "\n",
    "        # Train and tune using Autoensampler from optuna\n",
    "        else:\n",
    "            if not optuna_available:\n",
    "                raise Exception(\n",
    "                    \"optuna module is not available. Please install in order to use Autoensampler.\")\n",
    "            if not optunahub_available:\n",
    "                raise Exception(\n",
    "                    \"auto-sampler module is not available. Please install in order to use Autoensampler.\")\n",
    "\n",
    "            optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "            # Make classifiers\n",
    "            if self.model_choice == \"clf\":\n",
    "                if self.est_type == \"rdf\":\n",
    "\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _rdf_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._rdf_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making RandomForestClassifier using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"maximize\", study_name=\"randfor_clf_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best MSE is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_rdf(study.best_trial, X_train, y_train)\n",
    "\n",
    "                # XGBoost\n",
    "                else:\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _xgb_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._xgb_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making XGBClassifier using Autosampler Optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_clf_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_xgb(study.best_trial, X_train, y_train)\n",
    "\n",
    "            # Make regressors\n",
    "            else:\n",
    "\n",
    "                if self.est_type == \"rdf\":\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _rdf_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._rdf_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making RandomForestRegressor using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"randfor_reg_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "                    best_trials = study.best_trials\n",
    "                    print(f\"The number of best trials: {len(best_trials)}\")\n",
    "                    for trial in best_trials:\n",
    "                        print(f\"Trial {trial.number}:\\n\\tParams: {trial.params}\\n\\tValues: {trial.values}\")\n",
    "                    #self.best_params = study.best_params\n",
    "                    #self.model = self._get_rdf(study.best_trial, X_train, y_train)\n",
    "\n",
    "                elif self.est_type == \"xgb\":\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _xgb_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._xgb_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making XGBRegressor using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"xgb_reg_tuning\",  sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_xgb(study.best_trial, X_train, y_train)\n",
    "\n",
    "                elif self.est_type == \"svr\":\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _svr_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._svr_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making SVR using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"svr_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_svr(study.best_trial, X_train, y_train)\n",
    "\n",
    "                elif self.est_type == \"ridge\":\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _ridge_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._ridge_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making Ridge using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"ridge_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_ridge(study.best_trial, X_train, y_train)\n",
    "\n",
    "                elif self.est_type == \"lasso\":\n",
    "                    def obj(trial: optuna.Trial) -> float:\n",
    "                        \"\"\"This is a wrapper function n meant to call the hidden _lasso_obj method.\n",
    "                        Optuna requires a function call without any args\"\"\"\n",
    "                        return self._lasso_obj(trial, X_train, y_train)\n",
    "\n",
    "                    print(\"Making Lasso using Autosampler optimization...\")\n",
    "                    module = optunahub.load_module(package=\"samplers/auto_sampler\")\n",
    "                    study = optuna.create_study(direction=\"minimize\", study_name=\"lasso_tuning\", sampler=module.AutoSampler())\n",
    "                    print(\"Training and tuning...\")\n",
    "                    study.optimize(lambda trial: obj(trial=trial), n_trials=self.num_trials, n_jobs=-1, show_progress_bar=True)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(f\"The best score is {study.best_value}\")\n",
    "                    self.best_params = study.best_params\n",
    "                    self.model = self._get_lasso(study.best_trial, X_train, y_train)\n",
    "\n",
    "                else:\n",
    "                    print(\"Making making LinearRegression\")\n",
    "                    lin = LinearRegression(n_jobs=-1)\n",
    "                    print(\"Training...\")\n",
    "                    lin.fit(X_train, y_train)\n",
    "                    print(\"Training completed.\")\n",
    "                    print(\"Trained model saved as an attribute to class\")\n",
    "                    self.model = lin\n",
    "                    self.best_params = None\n",
    "        \n",
    "    def save_model(self, path_to_model:str) -> None:\n",
    "        \"\"\"This method saves the trained model to disk using the joblib library. NOTE:\n",
    "        the extension for the filename must be of the form filename.sav (i.e. it must\n",
    "        end with .sav)\n",
    "\n",
    "        Parameters:\n",
    "            - path_to_model (str): the path, containing the filename, to save the model to\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not yet been trained. Please train the model and re-execute this method\")\n",
    "\n",
    "        joblib.dump(self.model, path_to_model)\n",
    "        print(\"Model saved to \"+path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ```regression``` to make regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regression(Model, GeoPlotter):\n",
    "    \"\"\"This class is meant to create any regression model as specified by the user. It inherits from\n",
    "    the Model and GeoPlotter classes.\n",
    "\n",
    "    Attributes: \n",
    "        - est_type (str): the type of estimator to use for regression. See model documentation for all\n",
    "                          available estimators.\n",
    "        - tuning_strategy (str): the method to use for hyperparameter tuning. See model documentation for \n",
    "                                 all available tuning strategies.\n",
    "        - num_trials (int): number of trials for tuning.\n",
    "        - cv_fold (int): the number of folds to use for cross-validations.\n",
    "        - n_jobs (int): the number of parallel jobs to run.\n",
    "        - params (Dict): a dictionary of parameters to pass into the regression estimator.\n",
    "        - df (pd.DataFrame): the original dataframe (containing features, world names, and targets)\n",
    "        - happiness_predictions (ArrayLike): the happiness score predictions given by the regression\n",
    "                                             estimator.\n",
    "        - happiness_residuals (ArrayLike): the residuals between the happiness score predictions, as \n",
    "                                           given by the used estimator, and the actual happiness scores\n",
    "        \n",
    "\n",
    "    Methods:\n",
    "        - __init__(): the constructor\n",
    "        - get_happiness_predictions(): method to get the happiness score predictions\n",
    "        - get_happiness_residuals(): method to get the residuals of happiness scores\n",
    "        - get_worldplot(): the method to plot given data on a world plot\n",
    "        - plot_line(): the method to plot given x,y data on a 2D plot\n",
    "        - preds2csv(): the method to export given data to a csv file (using pandas)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, est_type:str='rdf', tuning_strategy:str='grid', num_trials:int=300, cv_fold:int=2,\n",
    "                 n_jobs:int=-1, params:Optional[Dict]=None, og_df:pd.DataFrame=None) -> None:\n",
    "        \"\"\"The constructor for the class. It set ups all attributes available for the class.\n",
    "\n",
    "        Parameters:\n",
    "            - est_type (str): the type of estimator to use for regression. Defaulted to 'rdf'. See model \n",
    "                             documentation for all available estimators.\n",
    "            - tuning_strategy (str): the method to use for hyperparameter tuning. Defaulted to 'grid'. \n",
    "                                 See model documentation for all all available tuning strategies.\n",
    "            - num_trials (int): number of trials for tuning. Defaulted to 300\n",
    "            - cv_fold (int): the number of folds to use for cross-validations. Defaulted to 4.\n",
    "            - n_jobs (int): the number of parallel jobs to run. Defaulted to -1 for all available processors.\n",
    "            - params (Dict): a dictionary of parameters to pass into the regression estimator. Defaulted to\n",
    "                             None.\n",
    "            - og_df (pd.DataFrame): the original dataframe unaltered from where all information was used to \n",
    "                                    make the ML model (containing features, world names, and targets)\n",
    "        \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "        \n",
    "        super(regression, self).__init__(est_type=est_type, tuning_strategy=tuning_strategy, num_trials=num_trials,\n",
    "                         cv_fold=cv_fold, n_jobs=n_jobs, params=params, df=og_df)\n",
    "        self.happiness_predictions = None\n",
    "        self.happiness_residuals = None\n",
    "\n",
    "    def get_happiness_predictions(self, X:NDArray, y_true:ArrayLike) -> None:\n",
    "        \"\"\"This function accepts a set data features and stores the predictions\n",
    "        of happiness-scores made by the model as an attributes. It also stores\n",
    "        the MSE and r-squared scores obtained from making the predictions.\n",
    "\n",
    "        Parameters:\n",
    "            - X (ArrayLike): the set of data features\n",
    "            - y_true (ArrayLike): the true happiness targets. Used to get \n",
    "                                  MSE and r-squared scores\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            raise Exception(\"Model has not yet been trained. Please train the model and re-execute this method\")\n",
    "\n",
    "        self.happiness_predictions = self.model.predict(X)\n",
    "        self.happiness_predictions_MSE = mean_squared_error(y_true=y_true, y_pred=self.happiness_predictions)\n",
    "        self.happiness_predictions_r2_score = r2_score(y_true=y_true, y_pred=self.happiness_predictions)\n",
    "\n",
    "    def get_happiness_residuals(self, y_true:ArrayLike) -> None:\n",
    "        \"\"\"This function accepts the true target values and stores the\n",
    "        residuals of the model.\n",
    "\n",
    "        Parameters:\n",
    "            - y_true (ArrayLike): the true target values\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            raise Exception(\"Model has not yet been trained. Please train the model and re-execute this method\")\n",
    "        elif self.happiness_predictions is None:\n",
    "            raise Exception(\"Model has not yet computed happiness-score predictions. Compute the predictions using \" +\\\n",
    "                            \"'.get_happiness_predictions(X)' then rerun this method.\")\n",
    "\n",
    "        self.residuals = y_true - self.happiness_predictions\n",
    "\n",
    "    def get_worldplot(self, y_data:ArrayLike=[], y_name:str=\"happiness predictions\", fig_title:str='title', \n",
    "                      save_fig:bool=False, path_to_fig:str='fig.pdf') -> None:\n",
    "        \"\"\"This function plots the given data into the world plot. It then saves the \n",
    "        created image, if specified, into a pdf format.\n",
    "        \n",
    "        Parameters:\n",
    "            - y_data (ArrayLike): the data to plot on the world map. Defaulted\n",
    "                                  to an empty list. Default value will plot\n",
    "                                  the happiness predictions.\n",
    "            - y_name (str): the name of the column of the original dataframe\n",
    "                            to plot on the worldmap. Defaulted to 'happiness\n",
    "                            predictions'.\n",
    "            - fig_title (str): the title to give the image. Defaulted to 'title'.\n",
    "            - save_fig (bool): whether to save the figure into a pdf format \n",
    "                               or not. Defaulted to False\n",
    "            - path_to_fig (str): the path, containing the filename, on where to save\n",
    "                                 the figure. Defauled to 'fig.pdf'\n",
    "        \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if y_name.strip().lower() == \"happiness predictions\":\n",
    "            y_data = self.happiness_predictions\n",
    "        \n",
    "        if (y_name.strip().lower() != \"happiness predictions\") and not y_data:\n",
    "            raise ValueError(\"Argument 'y_data' cannot be empty if not using default 'happiness prediction'.\")\n",
    "\n",
    "        self.df[y_name] = y_data\n",
    "        obj = GeoPlotter(df=self.df)\n",
    "        obj.plot(col_name=y_name, title=fig_title, save_img=save_fig, img_name=path_to_fig)\n",
    "        \n",
    "    def plot_line(self, y_data:ArrayLike, x_data:ArrayLike, xlabel:str='x', ylabel:str='y', title:str='Regression',\n",
    "                  save_line_plot:bool=False, path_to_fig:str='fig.pdf') -> None:\n",
    "        \"\"\"This function plots the given arrays in 2D plot. It also saves the image if \n",
    "        specified in pdf format.\n",
    "\n",
    "        Parameters:\n",
    "            - y_data (ArrayLike): the data to plot on the y-axis\n",
    "            - x_data (ArrayLike): the data to plot on the x-axis\n",
    "            - xlabel (str): the label for the x-axis. Defaulted to 'x'\n",
    "            - ylabel (str): the label for the y-axis. Defaulted to 'y'\n",
    "            - title (str): the title for the created image. Defaulted to 'Regression'.\n",
    "            - save_line_plot (bool): whether to save the created line plot or not. De-\n",
    "                                     faulted to False.\n",
    "            - path_to_fig (str): the path, containing the filename, on where to save\n",
    "                                 the figure. Defaulted to 'fig.pdf'.\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        ax = plt.subplot(111)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title)\n",
    "        ax.plot(x_data, y_data)\n",
    "\n",
    "        if save_line_plot:\n",
    "            plt.savefig(path_to_fig, format='pdf')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def preds2csv(self, preds:ArrayLike|NDArray=[], header:bool|List|str=False, \n",
    "                path_to_file:str=\"./happiness/data/rdf_reg_happ_preds.csv\") -> None:\n",
    "        \"\"\"This function exports given data to a csv file. By default, it exports the \n",
    "        happiness scores predictions if the 'preds' argument is empty.\n",
    "        \n",
    "        Parameters:\n",
    "            - preds (ArrayLike|NDArray): the data to export to csv. Defaulted to an\n",
    "                                        empty list.\n",
    "            - header (bool|List|str): the header to give the csv file. Defaulted to False.\n",
    "                                      It can be a boolean, a list, or a string.\n",
    "            - path_to_file (str): the path, including file name, to the directory in\n",
    "                                  where to store the exported csv file. Defaulted to\n",
    "                                  './data/happiness/rdf_reg_happ_preds.csv'\n",
    "            \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if len(preds) == 0:\n",
    "\n",
    "            # Check for predictions\n",
    "            if \"happiness predictions\" not in self.df.columns.to_list():\n",
    "                self.df[\"happiness predictions\"] = self.happiness_predictions\n",
    "\n",
    "            df = self.df[['ISO_A3', \"happiness predictions\"]]\n",
    "            df.to_csv(path_to_file, header=False)\n",
    "\n",
    "        else:\n",
    "            df = pd.DataFrame(preds)\n",
    "            df.to_csv(path_to_file, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_model():\n",
    "    \"\"\"This class is meant to load and help in using a trained ML model.\n",
    "\n",
    "    Attributes:\n",
    "        - model: the loaded trained model\n",
    "        - df: original unaltered dataframe from where the training and testing data come from\n",
    "        - happiness_predictions: the happiness score predictions made by the model\n",
    "        - residuals: the residuals between the happiness predictions and the true happiness\n",
    "                     scores\n",
    "    \n",
    "    Methods:\n",
    "        - __init__(): the constructor for the class\n",
    "        - get_happiness_predictions(): the method used to get happiness predictions\n",
    "        - get_happiness_residuals(): the method used to get happiness residuals\n",
    "        - get_worldplot(): the method used to create a world plot\n",
    "        - preds2csv(): the method used to export data to a csv file    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, path_to_model:str=MODELS_DIR+'./saved_models/rdf_reg_new.sav', og_df:pd.DataFrame=[]):\n",
    "        \"\"\"This function initiates the class and uploads the saved model as an attribute.\n",
    "        \n",
    "        Parameters:\n",
    "            - path_to_model (str): the path to the joblib (.sav) file containing the trained model. Defaul-\n",
    "                                   ted to MODELS_DIR+'rdf_reg_new.sav.sav' for a random forest regressor.\n",
    "            - og_df (pd.DataFrame): the original dataframe unaltered from where all information was used to \n",
    "                                    make the ML model (containing features, world names, and targets). Def-\n",
    "                                    aulted to an empty list.\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = joblib.load(path_to_model)\n",
    "        if isinstance(og_df, list):\n",
    "            raise ValueError(\"Must give the original dataframe when instatiating (i.e. constructor)!\")\n",
    "        self.df = og_df\n",
    "\n",
    "    def get_happiness_predictions(self, X:NDArray|pd.DataFrame) -> None:\n",
    "        \"\"\"This function accepts a set data features and stores the predictions\n",
    "        of happiness-scores made by the model as an attributes\n",
    "\n",
    "        Parameters:\n",
    "            - X (ArrayLike|pd.DataFrame): the set of data features\n",
    "            \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        self.happiness_predictions = self.model.predict(X)\n",
    "\n",
    "    def get_happiness_residuals(self, y_true:ArrayLike) -> None:\n",
    "        \"\"\"This function accepts the true target values and stores the\n",
    "        residuals of the model.\n",
    "\n",
    "        Parameters:\n",
    "            - y_true (ArrayLike): the true target values\n",
    "\n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.happiness_predictions is None:\n",
    "            raise Exception(\"Model has not yet computed happiness-score predictions. Compute the predictions using \" +\\\n",
    "                            \"'.get_happiness_predictions(X)' then rerun this method.\")\n",
    "\n",
    "        self.residuals = y_true - self.happiness_predictions\n",
    "    \n",
    "    def get_worldplot(self, y_data:ArrayLike=[], y_name:str=\"happiness predictions\", fig_title:str='title', \n",
    "                      save_fig:bool=False, path_to_fig:str=IMG_DIR+'fig.pdf') -> None:\n",
    "        \"\"\"This function plots the given data into the world plot. It then saves the \n",
    "        created image, if specified, into a pdf format.\n",
    "        \n",
    "        Parameters:\n",
    "            - y_data (ArrayLike): the data to plot on the world map. Defaulted\n",
    "                                  to an empty list. Default value will plot\n",
    "                                  the happiness predictions.\n",
    "            - y_name (str): the name of the column of the original dataframe\n",
    "                            to plot on the worldmap. Defaulted to 'happiness\n",
    "                            predictions'.\n",
    "            - fig_title (str): the title to give the image. Defaulted to 'title'.\n",
    "            - save_fig (bool): whether to save the figure into a pdf format \n",
    "                               or not. Defaulted to False\n",
    "            - path_to_fig (str): the path, containing the filename, on where to save\n",
    "                                 the figure. Defauled to IMG_DIR+'fig.pdf'\n",
    "        \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.happiness_predictions is None:\n",
    "            raise Exception(\"Model has not yet computed happiness-score predictions. Compute the predictions using \" +\\\n",
    "                            \"'.get_happiness_predictions(X)' then rerun this method.\")\n",
    "\n",
    "        if y_name.strip().lower() == \"happiness predictions\":\n",
    "            y_data = self.happiness_predictions\n",
    "        \n",
    "        if (y_name.strip().lower() != \"happiness predictions\") and not y_data:\n",
    "            raise ValueError(\"Argument 'y_data' cannot be empty if not using default 'happiness prediction'.\")\n",
    "\n",
    "        self.df[y_name] = y_data\n",
    "        obj = GeoPlotter(df=self.df)\n",
    "        obj.plot(col_name=y_name, title=fig_title, save_img=save_fig, img_name=path_to_fig)\n",
    "\n",
    "    def preds2csv(self, preds:ArrayLike|NDArray=[], header:bool|List|str=False, \n",
    "                  path_to_file:str=DATA_DIR+\"rdf_reg_happ_preds.csv\") -> None:\n",
    "        \"\"\"This function exports given data to a csv file. By default, it exports the \n",
    "        happiness scores predictions if the 'preds' argument is empty.\n",
    "        \n",
    "        Parameters:\n",
    "            - preds (ArrayLike|NDArray): the data to export to csv. Defaulted to an\n",
    "                                         empty list.\n",
    "            - header (bool|List|str): the header to give the csv file. Defaulte to False.\n",
    "                                      It can be a boolean, a list, or a string.\n",
    "            - path_to_file (str): the path, including file name, to the directory in\n",
    "                                  where to store the exported csv file. Defaulted to\n",
    "                                  DATA_DIR+'rdf_reg_happ_preds.csv'\n",
    "        \n",
    "        Returns:\n",
    "            - None\n",
    "        \"\"\"\n",
    "\n",
    "        if len(preds) == 0:\n",
    "\n",
    "            # Check for predictions\n",
    "            if \"happiness predictions\" not in self.df.columns.to_list():\n",
    "                self.df[\"happiness predictions\"] = self.happiness_predictions\n",
    "\n",
    "            df = self.df[['ISO_A3', \"happiness predictions\"]]\n",
    "            df.to_csv(path_to_file, header=False)\n",
    "\n",
    "        else:\n",
    "            df = pd.DataFrame(preds)\n",
    "            df.to_csv(path_to_file, header=header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACME310Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
