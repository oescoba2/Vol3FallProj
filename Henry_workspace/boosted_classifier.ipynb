{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/henryfetzer/Documents/ACME/Vol3FallProj/data/Life_Expectancy_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2938, 22)\n",
      "(1649, 20)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "df = df.rename(columns={'Life expectancy ': 'Life expectancy', 'under-five deaths ':'under-five deaths',\n",
    "                        'Measles ':'Measles'})\n",
    "df.drop(columns=['Country', 'Status'], inplace=True)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target column\n",
    "df['life_over_70'] = (df['Life expectancy'] >= 70).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1319, 18)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "data = df.drop(columns=['life_over_70', 'Life expectancy', 'Adult Mortality'])\n",
    "target = df['life_over_70']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV 1/2] END loss=log_loss, max_depth=3, min_samples_leaf=1, n_estimators=100;, score=0.930 total time=   0.7s\n",
      "[CV 2/2] END loss=log_loss, max_depth=3, min_samples_leaf=1, n_estimators=100;, score=0.957 total time=   0.7s\n",
      "[CV 1/2] END loss=log_loss, max_depth=3, min_samples_leaf=1, n_estimators=200;, score=0.934 total time=   1.6s\n",
      "[CV 1/2] END loss=log_loss, max_depth=3, min_samples_leaf=3, n_estimators=100;, score=0.929 total time=   0.9s\n",
      "[CV 2/2] END loss=log_loss, max_depth=3, min_samples_leaf=1, n_estimators=200;, score=0.962 total time=   1.7s\n",
      "[CV 2/2] END loss=log_loss, max_depth=3, min_samples_leaf=3, n_estimators=100;, score=0.950 total time=   0.9s\n",
      "[CV 2/2] END loss=log_loss, max_depth=5, min_samples_leaf=1, n_estimators=100;, score=0.966 total time=   1.5s\n",
      "[CV 1/2] END loss=log_loss, max_depth=5, min_samples_leaf=1, n_estimators=100;, score=0.951 total time=   1.5s\n",
      "[CV 1/2] END loss=log_loss, max_depth=3, min_samples_leaf=3, n_estimators=200;, score=0.938 total time=   1.8s\n",
      "[CV 2/2] END loss=log_loss, max_depth=3, min_samples_leaf=3, n_estimators=200;, score=0.959 total time=   1.8s\n",
      "[CV 1/2] END loss=log_loss, max_depth=5, min_samples_leaf=3, n_estimators=100;, score=0.955 total time=   1.5s\n",
      "[CV 2/2] END loss=log_loss, max_depth=5, min_samples_leaf=3, n_estimators=100;, score=0.961 total time=   1.5s\n",
      "[CV 1/2] END loss=log_loss, max_depth=5, min_samples_leaf=1, n_estimators=200;, score=0.952 total time=   3.0s\n",
      "[CV 2/2] END loss=log_loss, max_depth=5, min_samples_leaf=1, n_estimators=200;, score=0.970 total time=   3.1s\n",
      "[CV 1/2] END loss=exponential, max_depth=3, min_samples_leaf=1, n_estimators=100;, score=0.927 total time=   1.2s\n",
      "[CV 2/2] END loss=exponential, max_depth=3, min_samples_leaf=1, n_estimators=100;, score=0.946 total time=   1.3s\n",
      "[CV 1/2] END loss=log_loss, max_depth=5, min_samples_leaf=3, n_estimators=200;, score=0.959 total time=   3.0s\n",
      "[CV 2/2] END loss=log_loss, max_depth=5, min_samples_leaf=3, n_estimators=200;, score=0.963 total time=   3.1s\n",
      "[CV 1/2] END loss=exponential, max_depth=3, min_samples_leaf=3, n_estimators=100;, score=0.927 total time=   0.8s\n",
      "[CV 2/2] END loss=exponential, max_depth=3, min_samples_leaf=3, n_estimators=100;, score=0.950 total time=   0.8s\n",
      "[CV 1/2] END loss=exponential, max_depth=3, min_samples_leaf=1, n_estimators=200;, score=0.934 total time=   1.6s\n",
      "[CV 2/2] END loss=exponential, max_depth=3, min_samples_leaf=1, n_estimators=200;, score=0.957 total time=   1.6s\n",
      "[CV 1/2] END loss=exponential, max_depth=3, min_samples_leaf=3, n_estimators=200;, score=0.937 total time=   1.8s\n",
      "[CV 1/2] END loss=exponential, max_depth=5, min_samples_leaf=1, n_estimators=100;, score=0.946 total time=   1.7s\n",
      "[CV 2/2] END loss=exponential, max_depth=3, min_samples_leaf=3, n_estimators=200;, score=0.962 total time=   1.9s\n",
      "[CV 2/2] END loss=exponential, max_depth=5, min_samples_leaf=1, n_estimators=100;, score=0.966 total time=   1.7s\n",
      "[CV 1/2] END loss=exponential, max_depth=5, min_samples_leaf=3, n_estimators=100;, score=0.951 total time=   1.2s\n",
      "[CV 2/2] END loss=exponential, max_depth=5, min_samples_leaf=3, n_estimators=100;, score=0.967 total time=   1.3s\n",
      "[CV 1/2] END loss=exponential, max_depth=5, min_samples_leaf=1, n_estimators=200;, score=0.956 total time=   2.6s\n",
      "[CV 2/2] END loss=exponential, max_depth=5, min_samples_leaf=1, n_estimators=200;, score=0.967 total time=   2.6s\n",
      "[CV 1/2] END loss=exponential, max_depth=5, min_samples_leaf=3, n_estimators=200;, score=0.961 total time=   2.1s\n",
      "[CV 2/2] END loss=exponential, max_depth=5, min_samples_leaf=3, n_estimators=200;, score=0.966 total time=   2.0s\n",
      "{'loss': 'exponential', 'max_depth': 5, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "0.9635301100305894\n"
     ]
    }
   ],
   "source": [
    "#Grid search for best classifier\n",
    "# classifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "# specify hyperparameters\n",
    "param_grid = {\"alpha\": ['log_loss', 'exponential'],\n",
    "              \"gamma\": [3,5],\n",
    "              \"lambda\": [1,3],\n",
    "              \"eta\": [100,200]}\n",
    "gbc_gs = GridSearchCV(gbc, param_grid, scoring=\"f1\", n_jobs=-1, verbose=5, cv=2)\n",
    "\n",
    "# run the search\n",
    "gbc_gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# print best parameters and resulting score\n",
    "print(gbc_gs.best_params_, gbc_gs.best_score_, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income composition of resources: 0.556288852383641\n",
      " thinness 5-9 years: 0.07111483719764868\n",
      " HIV/AIDS: 0.06033280162929234\n",
      "Alcohol: 0.060010205360026186\n",
      " thinness  1-19 years: 0.053797069613392166\n",
      "Total expenditure: 0.041781470720396986\n",
      "under-five deaths: 0.025484754073220903\n",
      "infant deaths: 0.02520890565369538\n",
      "GDP: 0.02170682221624766\n",
      "Schooling: 0.018980604592068986\n",
      " BMI : 0.016746971683107112\n",
      "Year: 0.01347590212881509\n",
      "Hepatitis B: 0.010546794733175723\n",
      "Population: 0.006175538745162405\n",
      "Diphtheria : 0.005494308146426417\n",
      "Polio: 0.004608874539640368\n",
      "percentage expenditure: 0.004287169550525578\n",
      "Measles: 0.00395811703351698\n"
     ]
    }
   ],
   "source": [
    "# zip features importance and names together\n",
    "feature_names = data.columns\n",
    "features_and_importance = sorted(zip(gbc_gs.best_estimator_.feature_importances_, feature_names), \n",
    "                                 reverse=True)\n",
    "\n",
    "for importance, name in features_and_importance:\n",
    "    print(f\"{name}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look similar to those for the random forest. We don't yet have a higher level of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
